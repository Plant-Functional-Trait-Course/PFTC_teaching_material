---
editor_options: 
  markdown: 
    wrap: sentence
---

```{r, setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

library(tidyverse)
library(lubridate)

```


# Data curation

Data curation, transformation or cleaning is the first step after digitizing the data.
Each dataset has to be checked for errors and corrected as best as possible.
This tutorial shows how to check your dataset for errors and how to correct them.

For this tutorial we will be working with the trait dataset from Svalbard.
See chapter @sec-pftc-data for how to access the data and information about the study, experiment and datasets.


## Useful packages

There are a couple of R packages that are useful for this work. 
First, **tidyverse** is a collection of R packages used for basic data manipulation and analysis.

If you have never used the packages you need to install it first using the function  `install.packages("tidyverse")`.
Otherwise, you can just load the packages.

```{r, load-packages, eval=FALSE}

library(tidyverse)

```

Second, another useful package for data curation is **tidylog**, which is built on the **dplyr** and **tidyr** packages and provides useful information about the functions used.

Tidylog will for example tell you how many rows have been removed and are remaining when using the `filter()` function or how many rows match when using a join function.
The information is always indicated in absolute numbers and percentage.
The additional information is very useful to check if the right observations have been removed or manipulated, because mistakes are easily done.

Let's install and/or load **tidylog**.

```{r, load-tidylog, message=FALSE}

library(tidylog)

```


Note, that once **tidylog** is loaded it will automatically prioritize the tidylog function before the dplyr and tidyr functions.
You actively have to choose if you do not want to use the tidylog version by using this notation: `dplyr::filter()`.


## Import data

The first step is to import the data to R.
The data is stored as a _csv file_ and we can use the function `read_csv()` to import that data.
If your data has another format or importing data is new to, you have a look at this [page](https://biostats-r.github.io/biostats/workingInR/importing-data-in-r.html).

```{r, trait-import}

raw_traits <- read_csv("data/PFTC4_Svalbard_2018_Gradient_Traits.csv")

```

Give the dataset a name that indicates that this is the raw data.

The dataset contains measurements of 14 traits from two elevational gradients on Svalbard.
The traits were measured on individual plants from 21 different graminoid and forb species.
For more information about the sites, traits and measurements see [here](https://github.com/Plant-Functional-Trait-Course/PFTC_4_Svalbard).


**Some manipulation**

Let us introduce some errors to the dataset.

The code to do this is hidden.
But if you want to replicate the code to introduce errors you can find the code [here](https://github.com/Plant-Functional-Trait-Course/PFTC_teaching_material/R/manipulation.R).


```{r, introduce-errors, echo=FALSE, message=FALSE}

raw_traits <- raw_traits |> 
  # simplify
  select(-Project, -Functional_group, -Year) |> 
  #introduce errors
  # wrong date
  mutate(Date = as.character(Date),
         Date = if_else(ID == "AMO3822", "18", Date),
         # giant leaf
         Value = if_else(ID == "ANH3472" & Trait == "Leaf_Area_cm2", 17965, Value),
         # diverse species names
         Taxon = case_when(ID == "BON2388" ~ "oxiria digyna",
                           ID == "ATX7216" ~ "oxyria digina",
                           ID == "BSV7581" ~ "oxyra digyna",
                           ID == "BUF9439" ~ "calalmagrostis neglecta",
                           TRUE ~ Taxon),
         # introduce NAs
         Value = if_else(ID %in% c("BLQ1061", "AZO1656", "AMV4451") & Trait == "LDMC", NA_real_, Value))
  
# add duplicate row
duplicate <- raw_traits |> 
  slice(378)

# introduce dry weight measurements with wrong unit (mg instead of g)
wrong_decimal <- raw_traits |> 
  filter(Trait == "Dry_Mass_g") |> 
  sample_n(size = 50, replace = FALSE) |> 
  mutate(Value2 = Value / 1000)

raw_traits <- raw_traits |> 
  left_join(wrong_decimal, by = c("Date", "Gradient", "Site", "PlotID", "Individual_nr", "ID", "Taxon", "Trait", "Value", "Elevation_m", "Latitude_N", "Longitude_E")) |> 
  mutate(Value = if_else(!is.na(Value2), Value2, Value)) |> 
  select(-Value2) |> 
  bind_rows(duplicate)

```


## First check of the dataset

By typing `raw_traits` in the console it will display the first rows and columns of the dataset.

```{r, display-table}

raw_traits

```

At the top you can see that the dataset has `r dim(raw_traits)[1]` observations and `r length(raw_traits)` columns.
These numbers give you a first impression if you have imported the right file, and if all your observations and columns are there.


### Check the data type

The next thing to check is if the variables have the right data type (or class in R terminology).
For each variable the output indicates the data type. The most common types are _dbl_ (numeric or integer), _chr_ (character), or _date_ (date).

If you want to know more about data types see [here](https://biostats-r.github.io/biostats/workingInR/first-steps-in-r.html#data-types-and-objects).

Now we want to check if the variables have the right data type.
The first variable *Date* is a character, which is not correct.
This probably means that one or several observations have a wrong date.
Let us check all different values for the variable *Date*.
For this we can use the function `distinct()` on the variable *Date*.

```{r, distinct}

raw_traits |> 
  distinct(Date)

```

We can see that there are 6 distinct dates in this variable.
One of the dates is "18", which is not a correct date format and turned the variable into a character.
Note that the additional information from the tidylog package about the `distinct()` function.
It shows the number of rows removed and remaining.

The next step is to check where the problem occurred.
For this we can use the function `filter()` to extract all observations with the date 18.
We can use `as.data.frame()` to display the whole table.

```{r, filter, eval=FALSE}

raw_traits |> 
  filter(Date == "18") |> 
  View()

```


```{r, filter-print, echo=FALSE}

raw_traits |> 
  filter(Date == "18") |> 
  print()


```


We see that it is a single observation (with multiple traits) that has the wrong date.
The next step is to check the raw data, notes, photos, etc. to find the correct date for this observations.
It is important to keep the data entry sheets, take a photo of them and keep the field notes to be able to fix such problems.

Luckily, we took pictures from all envelopes, and can have a look.
It seems to be a typo, because the data should be 18. July 2018 (@fig-envelope).

```{r}
#| label: fig-envelope
#| fig-cap: "The envelope with the leaf that has the wrong date."
#| fig-alt: "A photo showing the envelope with the leaf that has the wrong date."

knitr::include_graphics("images/resources/envelope.png")

```


Let's replace this value and give the variable the right class.

For this we will use the function `mutate()` which adds or manipulates a column.
Inside the mutate we will use the `if_else()` function to replace the date for a specific ID.
This function is useful for a single condition.
However for multiple statements (many if else conditions), we recommend to use the `case_when()` function (see below).
To give the variable the correct class, we use the `ymd()` function from the lubridate package.
Note that we now have to assign the table to a new or the same name to make the change permanent.

```{r, replace-date}

raw_traits <- raw_traits |> 
  mutate(Date = if_else(ID == "AMO3822", "2018-07-18", Date)) |> 
  mutate( Date = ymd(Date))
        
```


An important step when cleaning data is to check that you have done the right thing.
The **tidylog** functions show that for 7 observation Date has been changed.
This matches with the number of observations that had a wrong date.

To be sure we can look at the specific leaf (ID == "AMO3822") and see if the date is now corrected.
Another way would be to run the `distinct(Date)` function again. 

```{r, chacke-date}

raw_traits |> 
  filter(ID == "AMO3822") |> 
  select(Date)

```

The date has been fixed.

## Exercise {- .exercise .toc-ignore}

Now it is your turn.
Check if the data type for the variable *Date* is now correct.

<details>
  <summary>Hint</summary>
- type `raw_traits` to look at the whole dataset where the datatype of each variable is indicated
- use `class(raw_traits$Date)` which will tell you directly what type of class the variable has
- use `map(raw_traits, class)` to get the class of all variable in the dataframe
</details>


## Check for duplicates

Another common problem is duplicate observations.
This can happen when data is entered twice.
The why to find duplicates is to check that the combination of variables are unique.
In our dataset, we expect that *Date*, *Gradient*, *Site*, *PlotID*, *Individual_nr*, *ID*, *Taxon* and *Trait* should be unique, and only occurring once.

To check this, we can `group_by()` these variables and `filter()` for observations that occur more than once.

```{r, check-duplicates}

raw_traits |> 
  group_by(Date, Gradient, Site, PlotID, Individual_nr, ID, Taxon, Trait) |> 
  filter(n() > 1)

```

There is one duplicate entry.

Note that *Value* was not included in the `group_by()`.
This was done intentionally, because a common mistake is to have a duplicate, but with a different value.
This is either because one of the variables is wrong, e.g. it has the wrong Site and therefore appears to be a duplicate.
Alternatively, the leaf could have been measured twice by accident, which would likely give two slightly different values.
When getting a duplicate, these different options for why there is a duplicate have to be considered and carefully checked in the raw data.

In this case, we will assume that the leaf has only been measured once, but the data has been entered twice.
Thus, the two entries are exact duplicates.

To fix the duplicate problem, we group by the variables we expect to be unique.
Then we use `distinct()` with the argument *.keep_all = TRUE* to remove the duplicates.


```{r, remove-duplicates}

raw_traits2 <- raw_traits |> 
  group_by(Date, Gradient, Site, PlotID, Individual_nr, ID, Taxon, Trait) |> 
  distinct(.keep_all = TRUE)

```

Tidylog allows us to see what happens and how many rows have been removed.
There are 8 grouping variables and as expected, one row is filtered away, which is the duplicated row.

We can also run the code from above again to check if the duplicate is gone.


## Check for missing data

A common problem in a dataset are missing values.
Let's find out if we have any NAs in the dataset and if yes how many.

A quick way to get an overview of all NAs in the dataset is to use the following code:

```{r}

raw_traits %>% 
  select_if(function(x) any(is.na(x))) %>% 
  summarise_each(list(~sum(is.na(.))))
```

We can see that three variables, Date, PlotID and Value have NAs.
Let's focus on the missing values in Value.

Once the missing values are detected one has to decide if the missing data can be recovered, or if the missing values should be removed from the dataset.


```{r, remove-na}

raw_traits <- raw_traits |> 
  drop_na(Value)

```

This operation has removed 3 rows, which is the number of NA's in the dataset.


### Check values within variables

A common problem is inconsistencies within variables.
For example one of the variables in this dataset is `Taxon`.
It is very common to write latin species names wrong if they are not carefully checked or to make typos when entering the data.

Let's look at all unique species names using `distinct()` and sort them by Taxon using `arrange()`.

```{r, unique-names}

raw_traits |> 
  distinct(Taxon) |> 
  arrange(Taxon) |> 
  print(n = Inf)

```

There are four different versions for *oxyra digyna* and two for *calamagrostis neglecta*.
Obviously, some typos where made when entering the data.

Because we have to change multiple species names, we will use `case_when()`.

```{r, fix-oxyra}

raw_traits <- raw_traits |> 
  mutate(Taxon = case_when(Taxon %in% c("oxiria digyna", "oxyria digina", "oxyra digyna") ~ "oxyria digyna",
                           Taxon == "calalmagrostis neglecta" ~ "calamagrostis neglecta",
                           TRUE ~ Taxon))

```

An alternative to using `case_when()` to fix the problem, would be to create a dictionary with wrong and valid species names, which could be used to replace bad names.
This would be the better approach if many species names have to be changed.


## Checking Taxon using TNRS 

TNRS is a Taxonomic Name Resolution Service.

Maitner please add an example using TNRS.



## Visualise data

Some errors and problems in the data are difficult to detect by looking at the dataset.
For example checking if the measurements are realistic is nearly impossible by going through a table with numbers.
For this, visualising the data is much more effective.


### Histogram or density plot

Using histograms or denisty plots shows you the range of values a variable has.

```{r, hist}
#| label: fig-hist
#| fig-cap: "Density distributions of all measured traits."
#| fig-alt: "A plto showing the density distributions of all measured traits."
#| 
raw_traits |> 
  ggplot(aes(x = Value, fill = Gradient)) +
  geom_density(alpha = 0.7) +
  scale_fill_manual(values = c("green4", "grey")) +
  facet_wrap(~ Trait, scales = "free")

```


Note that the size traits (plant height, leaf mass, area and thickness) have distributions with very long tails.
This is common for size related variables and usually log transformation is a way to see the full range of these variables.

Leaf area has a huge tail and goes up to almost 20'000 cm^2^.
This is a leaf of almost 2 m^2^, which is impossible for a plant from Svalbard.
This value needs to be checked, it could be a typo.

Let's log transform the size traits.

```{r, log-transform}

raw_traits <- raw_traits |> 
  mutate(Value_log = if_else(Trait %in% c(
    "Plant_Height_cm",
    "Wet_Mass_g",
    "Dry_Mass_g",
    "Leaf_Area_cm2",
    "Leaf_Thickness_mm"), log(Value), Value),
    Trait = recode(Trait,
                   "Plant_Height_cm" = "Plant_Height_cm_log",
                   "Wet_Mass_g" = "Wet_Mass_g_log",
                   "Dry_Mass_g" = "Dry_Mass_g_log",
                   "Leaf_Area_cm2" = "Leaf_Area_cm2_log",
                   "Leaf_Thickness_mm" = "Thickness_mm_log"))

```

And replot the density plot.

```{r, hist-log}
#| label: fig-hist-log
#| fig-cap: "Density distributions of all measured traits."
#| fig-alt: "A plto showing the density distributions of all measured traits."
#| 
raw_traits |> 
  ggplot(aes(x = Value_log, fill = Gradient)) +
  geom_density(alpha = 0.7) +
  scale_fill_manual(values = c("green4", "grey")) +
  facet_wrap(~ Trait, scales = "free")

```

We can now better see the full range of the size traits.

We should now have a look at the giant leaf.

```{r, giant-leaf}
raw_traits |> 
  filter(Trait == "Leaf_Area_cm2_log",
         Value > 10)

```

One leaf is huge and 

```{r, fix-giant-leaf}
raw_traits <- raw_traits |> 
  mutate(Value = if_else(ID == "ANH3472" & Trait == "Leaf_Area_cm2_log", 1.7965, Value),
         Value_log = if_else(ID == "ANH3472" & Trait == "Leaf_Area_cm2_log", 0.5858403, Value_log))

```


### Correlations

Another way to check the data is to plot variables against each other that should be correlated.
In this dataset, we can plot dry mass against leaf area.
We would expect a positive correlation between the two variables, where large leaves have a higher dry mass.

```{r}
#| label: fig-correlation
#| fig-cap: "Correlation between leaf area and dry mass."
#| fig-alt: "A plto showing the correlation between leaf area and dry mass.."
#| 
raw_traits |> 
  pivot_wider(names_from = Trait, values_from = Value_log) |> 
  ggplot(aes(x = Dry_Mass_g_log, y = Leaf_Area_cm2_log)) +
  geom_point()

```
