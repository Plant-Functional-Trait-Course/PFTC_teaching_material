[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PFTC Teaching Material",
    "section": "",
    "text": "This book provides the teaching material for the Plant Functional Trait Courses (PFTC).\nThe PFTC courses are an international course with hands-on training in trait-based ecology. For more information about the courses see our course webpage.\nHere we provide teaching material for data collection, curation, analysis, documentation, storage, and reuse. In addition, we show reproducible and transparent workflows to make research more open and available. We cover these topics within the fields of plant functional trait ecology, ecosystem ecology, plant physiology and remote sensing.\n\n\n\n\nFigure 1: Trait data collection during PFTC4 on Svalbard."
  },
  {
    "objectID": "1_courses.html",
    "href": "1_courses.html",
    "title": "\n1  PFTC Course information\n",
    "section": "",
    "text": "The PFTC6 course will run from 23. July - 5. August 2022 and will be held in Aurland, western Norway. The location is near Bergen where the course team will be assembling before traveling to Aurland. Transport from Bergen to Aurland (by bus or car) can be organized by the course leaders. If you arrive via Oslo or elsewhere, transport to Aurland (by train or bus) needs to be organized by yourself. All flights to Bergen should be scheduled to arrive at the latest on 22nd of July and depart at the earliest on 6th of August from Bergen (other travel needs to arrive in Aurland on the 23rd at the latest, and depart on the 5th at the latest).\n\nknitr::include_graphics('images/sites/aurland.jpg')\n\n\n\nFigure 1.1: The fjords in Aurland in Western Norway."
  },
  {
    "objectID": "1_courses.html#previouse-courses",
    "href": "1_courses.html#previouse-courses",
    "title": "\n1  PFTC Course information\n",
    "section": "\n1.2 Previouse courses",
    "text": "1.2 Previouse courses\nPreviously, we have held courses in China (PFTC1 and 2), Peru (PFTC3 and 5) as well as on Svalbard (PFTC4)."
  },
  {
    "objectID": "2_study_system.html",
    "href": "2_study_system.html",
    "title": "\n2  Alpine ecosystems\n",
    "section": "",
    "text": "The PFTC courses study trait-based approaches in alpine grassland ecosystems around the world. We have chosen this ecosystem because they are impacted more strongly by climate change which cause important issues of great societal consequence, and because we have research backgrounds in this general field and system. By focusing on the same ecosystem in all courses, enables comparative approaches by combining data from the different PFTC courses. And also, everyone knows grasses are the coolest plants."
  },
  {
    "objectID": "2_study_system.html#alpine-ecosystems-in-western-norway-pftc6",
    "href": "2_study_system.html#alpine-ecosystems-in-western-norway-pftc6",
    "title": "\n2  Alpine ecosystems\n",
    "section": "\n2.1 Alpine ecosystems in Western Norway (PFTC6)",
    "text": "2.1 Alpine ecosystems in Western Norway (PFTC6)\nThe PFTC6 will be held in Aurland in the fjords of Western Norway (see Figure 2.1). We will be working on an elevational gradient that ranges between 500 and 1300 m a.s.l.\n\n\n\n\nFigure 2.1: Map of the study area and sites in Aurland, Norway.\n\n\n\n\n\n\n\n\nFigure 2.2: The fjords in Aurland in Western Norway.\n\n\n\n\nThe dominant vegetation types in this area are grasslands, pine and birch forests with bilberry understory, and heathland. The main land use in this area is grazing by sheep and goats.\nDuring the course, we will mainly focus on grassland ecosystems (see Figure 5.3 (a) and Figure 5.3 (b)).\n\n\n\n\n\n(a) Alpine grassland\n\n\n\n\n\n\n(b) Lowland pasture\n\n\n\n\nFigure 2.3: A typical alpine grasslands and one at lower elevation."
  },
  {
    "objectID": "3_three_d.html",
    "href": "3_three_d.html",
    "title": "\n3  Three-D project\n",
    "section": "",
    "text": "The aim of the ThreeD project is to study global change impacts on biodiversity and ecosystem carbon fluxes. For a complete description of the experiment see here."
  },
  {
    "objectID": "3_three_d.html#study-sites",
    "href": "3_three_d.html#study-sites",
    "title": "\n3  Three-D project\n",
    "section": "\n3.1 Study sites",
    "text": "3.1 Study sites\nThe study is conducted in three semi-natural grassland sites in western Norway. The sites are location on calcareous soil (loamy sand). The sites were chosen to fit within an elevational gradient with three temperature levels that differ in c. 400 m elevation, a temperature difference of approximately 3°C in summer temperature (Figure 3.1). The sites correspond to boreal, sub-alpine and alpine biogeographic zones. Other factors such as history, bedrock, vegetation type and structure, slope and exposure were kept as constant as possible among the selected sites within each country. The sites are grazed by sheep and goats.\n\n\n\n\nFigure 3.1: A) The experiment are set up along an elevational gradient that represents a productivity gradient. The experimental treatments are warming (brown), a grdient of nitrogen addition (grey to green) and their combination. These treatments are crossed with four grazing treatment: control (no clipping), intermediate and intensive clipping, and natural grazing outside the grazing exclosure. B) Example of a plot divided into non destructive (inner square) and and destructive sampling area (outer square).\n\n\n\n\nThe lowest site Vikesland is locate at 469 m a.s.l. (60.9°N, 7.2°E; Figure 3.2). It is located near a farm, with 200 goats. In summer, these goats are moving up and down the mountains each day, to graze at higher elevation and being milked at the farm in the valley. The vegetation is dominated by Agrostis capillaris, Anthoxantum odoratum, Achillea millefolium, Ranunculus acris, and Rumex acetosa.\n\n\n\n\nFigure 3.2: The lowest site at the farm Vikesland.\n\n\n\n\nThe middle site Joasete is a summer farm and located at 920 m a.s.l (Figure 3.3). The area around the summer farm is mostly grazed by sheep and sometimes goats. The vegetation at Joasete is well adapted to the grazing pressure and nutrient input by the animals. The dominant plant species are Agrostis capillaris, Deschampsia cespitosa, Achillea millefolium, Ranunculus acris, Ranunculus repens, and Rumex acetosa.\n\n\n\n\nFigure 3.3: The middle site at the summer farm Joasete.\n\n\n\n\nThe highest site Liahovden is located at 1290 m a.s.l (Figure 3.4). It is grazing moderately by sheep, deer and reindeer. The vegetation is a species rich grassland, typical for nutrient soils in Norwegian alpine vegetation with Achillea millefolium, Antennaria dioica, Bistorta vivipara, Leontodon autumnalis, Silene acaulis, and Thalictrum alpinum.\n\n\n\n\nFigure 3.4: The higsest site at Liahovden."
  },
  {
    "objectID": "3_three_d.html#experimental-treatments",
    "href": "3_three_d.html#experimental-treatments",
    "title": "\n3  Three-D project\n",
    "section": "\n3.2 Experimental treatments",
    "text": "3.2 Experimental treatments\nAt each site, ten blocks, with 8 plots (50 x 50 cm) were marked in each corner. Six plots were chosen relatively close to each other (inside the fence) and two plots were chosen further away (outside fence). Each plot was given a unique originPlotID starting at the upper left corner in block 1 and the highest site. The numbering was continued to 160. After transplanting, each turf also received a unique destinationPlotID (a number from 1-200; see Figure 3.1 and below). Each plot received a combination of each treatment randomly (warming, nitrogen addition and grazing, see below). The randomization of the treatments was done in two steps. First, the 10 nitrogen levels were assigned randomly per block (i.e. block 1 received the same nitrogen level in each site). We chose to randomize the nitrogen treatment at the block level to avoid nitrogen contamination between the plots within the blocks. Second, the warming and grazing treatments were randomized within block. The six plots within each block (inside the fence) were randomly assigned a warming and grazing treatment. The two plots per block located outside the fence, were randomly assigned a warming treatment.\n\n3.2.1 Warming treatment\nThe warming treatment was conducted at the end of the growing season in 2019, by transplanting entire plant communities to lower elevation (c. +3°C; Figure 3.1). The upslope left-hand corner of each turf was marked with a toothpick that the turfs could be placed in the similar position relative to the slope and block orientation at the destination site. We used a knife to cut the turfs to 50 x 50 cm and to a depth of c. 10 - 25 cm (Figure 3.5), unless the soil was shallower, as was the case for some of the alpine plots. After excavation, the turfs were packed into a cardboard box and transported to their respective target sites within one day. The control plots were not excavated or moved. We did not have local transplants, to control for the transplanting effect, because this was tested in a previous project and the transplanting did not affect species composition in Norway (Vandvik et al. 2020) or China (Yang et al. 2018).\n\n\n\n\nFigure 3.5: A turf that is being transplanted.\n\n\n\n\nThe turfs were fitted into the gaps created by excavating turfs at the destination site, except for the low site, where gaps had to be made. Each block received one plot of each treatment. Turfs were placed in the block with the same numerical value (1 - 10) and nitrogen level in the destination site as they originated from in the origin site. Transplanted turfs were carefully planted into their destination plots (destinationPlotID) ensuring that the turf orientation was correct (using the toothpick marking the uphill left corner of each turf) and that the soil surface was in plane with the surrounding vegetation, and that the edges of the excavated plot was in good contact with the edges of the gap created when the original turf was excavated from the plot (Figure 3.6). If necessary, loose soil was carefully removed from the underside of the turf, or local soil was added to the gap or around the edges to achieve this.\n\n\n\n\nFigure 3.6: Turfs that have been transplanted from Liahovden to lower elevation at Joasete.\n\n\n\n\n\n3.2.2 Nitrogen addition\nIn each plot we added slow dissolving fertilizer as pellets (YaraBela OPTI-NS 27-0-0 (4S)). We used oxidised nitrogen (NO and N2O) formed mainly by combustion processes, which are the main sources of atmospheric nitrogen deposition in remote regions (i.e., away from intensive agriculture and other sources or reduced nitrogen). The fertilizer was added once at the start and once in the middle of the growing season from 2020 - 2022. Each block received one of the seven nitrogen levels: 0, 0.5, 1, 5, 10, 50, 100, 150 kg N ha−1 yr−1. Three of the blocks were controls and received 0 kg N ha−1 yr−1.\nThe natural nitrogen deposition in Norway is 1.5-3.5 kg N ha−1 yr−1. The critical load for changing species composition in these alpine grasslands is approximately 5-10 kg N ha−1 yr−1 in Norway. We therefore wanted to have a range of nitrogen levels that were below and well above this critical load.\n\n3.2.3 Grazing treatment\nThe warming and nitrogen treatments were crossed with four grazing treatments. Grazing was simulated by clipping the vegetation manually with scissors 2-3 cm above the ground (Figure 3.7). The four grazing treatments were natural grazing (N; outside the fence), medium level of grazing (M; 2x clipp), intensive level of grazing (I; 4x clipp), and untreated control plots (C). The intermediate clipping level reflects the natural grazing level outside the exclosure and should control for differences between grazing and clipping (i.e. clipping is not selective and will not add faeces and urine). The medium and intensive plots were clipped 2 or 4 times during the growing season. The clipping treatment was conducted in 2020 - 2022.\n\n\n\n\nFigure 3.7: A plot that has been clipped and the biomass sorted to functional groups."
  },
  {
    "objectID": "3_three_d.html#references",
    "href": "3_three_d.html#references",
    "title": "\n3  Three-D project\n",
    "section": "\n3.3 References",
    "text": "3.3 References\n\n\n\n\nVandvik, Vigdis, Olav Skarpaas, Kari Klanderud, Richard J Telford, Aud H Halbritter, and Deborah E Goldberg. 2020. “Biotic Rescaling Reveals Importance of Species Interactions for Variation in Biodiversity Responses to Climate Change.” Proc. Natl. Acad. Sci. U. S. A. 117 (37): 22858–65.\n\n\nYang, Yan, Aud Helen Halbritter, Kari Klanderud, Richard J Telford, Genxu Wang, and Vigdis Vandvik. 2018. “Transplants, Open Top Chambers (OTCs) and Gradient Studies Ask Different Questions in Climate Change Effects Studies.” Front. Plant Sci. 9: 1574."
  },
  {
    "objectID": "4_incline.html",
    "href": "4_incline.html",
    "title": "\n4  INCLINE project\n",
    "section": "",
    "text": "The aim of the INCLINE project is to …"
  },
  {
    "objectID": "4_incline.html#study-sites",
    "href": "4_incline.html#study-sites",
    "title": "\n4  INCLINE project\n",
    "section": "\n4.1 Study sites",
    "text": "4.1 Study sites\n\n\n\n\nFigure 4.1: The experiment is set up along a precipitation gradient in South Western Norway, from the driest site Ulvehaugen, in the east, to the wettest site Skjellingahaugen in the west. At each site, an experimental warming treatment has been imposed on alpine grassland vegetation using Open Top Chambers (OTCs), and paired control plots. There are seven sets of OTCs and control plots at each site."
  },
  {
    "objectID": "4_incline.html#experimental-treatments",
    "href": "4_incline.html#experimental-treatments",
    "title": "\n4  INCLINE project\n",
    "section": "\n4.2 Experimental treatments",
    "text": "4.2 Experimental treatments"
  },
  {
    "objectID": "4_incline.html#references",
    "href": "4_incline.html#references",
    "title": "\n4  INCLINE project\n",
    "section": "\n4.3 References",
    "text": "4.3 References"
  },
  {
    "objectID": "5_trait_collection.html",
    "href": "5_trait_collection.html",
    "title": "\n5  The trait wheel\n",
    "section": "",
    "text": "This protocol shows how to collect, measure, check and document leaf functional traits for vascular plants using the trait wheelTW (Figure 5.1). The protocol is based on the trait handbook from Perez-Harguindeguy et al. (2013)."
  },
  {
    "objectID": "5_trait_collection.html#collecting-leaves-in-the-field",
    "href": "5_trait_collection.html#collecting-leaves-in-the-field",
    "title": "\n5  The trait wheel\n",
    "section": "\n5.1 Collecting leaves in the field",
    "text": "5.1 Collecting leaves in the field\nBe careful at all time when working in the experiment and avoid stepping on the experimental plots. Make yourself familiar with the experimental design (check the site maps), and how plot, blocks and sites are marked.\nTo collect plants, go to your plot and use a sitting mat to sit next to the plot. Collect the leaves and minimize disturbing the surroundings as much as possible.\nWe will collect traits in three different locations/experiments:\n\nAt four sites along an elevational gradient (500 - 1300 m a.s.l.), with and without grazing\nIncline project: four sites with ambient and warming using OTCs\nThreeD project: two sites with different global change treatments including ambient/warming using transplants, nitrogen addition and grazing\n\nThe specific sampling and number of species will vary between the different projects, but we will focus on the following alpine and general species:\n\n\n\n\n\n Taxon \n    Type \n  \n\n\n Sibbaldia procumbens \n    alpine \n  \n\n Veronica alpina \n    alpine \n  \n\n Bistorta vivipara \n    alpine \n  \n\n Salix herbacea \n    alpine \n  \n\n Alchemilla alpina \n    alpine \n  \n\n Agrostis capillaris \n    alpine \n  \n\n Anthoxanthum odoratum \n    alpine \n  \n\n Carex bigelowii \n    alpine \n  \n\n Poa alpina \n    alpine \n  \n\n Thalictrum alpinum \n    alpine \n  \n\n Festuca rubra \n    alpine \n  \n\n Saussurea alpina \n    alpine \n  \n\n Achillea millefolium \n    general \n  \n\n Agrostis capillaris \n    general \n  \n\n Campanula rotundifolia \n    general \n  \n\n Carex vaginata \n    general \n  \n\n Leontodon autumnalis \n    general \n  \n\n\n\n\nIntraspecific group\nNOT FINAL PROTOCOL\nFollow the species list for our group, starting with the highest priority species for each site. DO NOT collect plants from within the plots or the trampled zone around the plots. Find 10 individuals of each species, ideally each individual should have five leaves, but three leaves is a minimum. Each individual should be at least two meters away from each other, to avoid sampling individuals from the same genetic individual. Choose reproductively mature adults with fully expanded leaves (i.e. not seedlings). Choose individuals with healthy-looking leaves (i.e. not grazed, no signs of pathogen or herbivore attack, no discoloration). Before uprooting the individual, measure its height (see below). For very large individuals that are too large to uproot (e.g. ferns), collect 5 leaves from the individual, making sure to include the petiole. If you are uncertain whether something is a leaf vs. leaflet, ask your group leader. If you cannot find 10 individuals of each species, that is ok. After a reasonable search, move on.\nGlobal change group and elevational gradient\nFor the ThreeD project we will collect the following treatments: - 3 ambient controls - 3 warm controls - ambient natural grazing - ambient and warm N levels: 5, 10, 50 and 150\nEach plot is 50 x 50 cm large and has an inner and outer part (see Figure 5.2). The plots are marked with metal tubes in each corner and a label on the top. The leaves should be collected from the outer part (destructive part) and if possible the inner part should stay untouched.\nFor the elevational gradient, we will in addition collect leaves at Høgsete and Vikelsand inside and outside the fence. For this, we will collect species according to the seedclim controls. The plants will not be collected from the plots, but from within the blocks, and from outside the fence.\n\n\n\n\nFigure 5.2: A) The experiment are set up along an elevational gradient that represents a productivity gradient. The experimental treatments are warming (brown), a grdient of nitrogen addition (grey to green) and their combination. These treatments are crossed with four grazing treatment: control (no clipping), intermediate and intensive clipping, and natural grazing outside the grazing exclosure. B) Example of a plot divided into non destructive (inner square) and and destructive sampling area (outer square).\n\n\n\n\nWe aim to collect leaves from the species that make up 80% of the cover in each plot. For each plot we will generate a species list. Below is an example code to generate such a species list:\n\nthreshold <-  80\n\ncover |> \n  select(turfID, species, cover) |> \n  group_by(turfID) |> \n  arrange(turfID, -cover) |> \n  mutate(cumsum = cumsum(cover)) |> \n  filter(cumsum <= threshold)\n\nFor each plot and species find 5 individuals that are spread across the plot to avoid sampling clones. Choose reproductively mature adults with fully expanded leaves (i.e. not seedlings) and healthy-looking leaves (i.e. not grazed, no signs of pathogen or herbivore attack, no discoloration).\nFor the grazing treatments, try to find mature and fully grown leaves if possible.\nMeasure the height of each individual plant and note down the height on the zip-loc bag (see below).\nCollect 1 leaf from the individual, making sure to include the petiole (Figure 5.3 (a)). If you are uncertain whether something is a leaf vs. leaflet (Figure 5.3 (b)), ask your group leader.\nIf you cannot find 5 individuals of each species, that is ok. After a reasonable search, move on to the next species or plot.\n\n\n\n\n\n(a) Leaf atanomy of a single leaf\n\n\n\n\n\n\n(b) A leaf with leaflets\n\n\n\n\nFigure 5.3: Leaf atanomy\n\n\nMeasuring vegetative height of individual plants in the field\nMeasure the vegetative height of the individual plant in the field (Figure 5.4). Vegetative height is the shortest distance between the main photosynthetic tissue on the plant and the ground, excluding any reproductive structure (e.g. buds, flowers or fruits). Leave the plant in it’s natural form and do not stretch the plant if it bends.\nDifferent plants will require different measures of height. Figure 5.4 shows a couple of typical examples. For cushion plants (A), stick a the ruler into the cusion all the way to the ground. For forbs and graminoids (B, D, E) measure the main stem leaves, and do not stretch the leaves (D, F, G ).\n\n\n\n\nFigure 5.4: Different plant types and how to measure height.\n\n\n\n\nRecord this measurement using legibly numbers (Figure 5.5) on the zip-loc bag. Place the leaf/leaves for each individual in a bag with wet paper towels for transfer to lab.\n\n\n\n\nFigure 5.5: The one and only approved way to write numbers!"
  },
  {
    "objectID": "5_trait_collection.html#preparing-leaves-for-measuring-traits-in-lab",
    "href": "5_trait_collection.html#preparing-leaves-for-measuring-traits-in-lab",
    "title": "\n5  The trait wheel\n",
    "section": "\n5.2 Preparing leaves for measuring traits in lab",
    "text": "5.2 Preparing leaves for measuring traits in lab\nSelect one (for the global change group) or five (for the intraspecific group) leaf/leaves per individual. For analysing chemical traits such as percent P, C, N and isotopes, a certain amount of dry biomass is needed. Make sure that we are collecting 0.03 g (30 mg) dried leaf material. Otherwise select several leaves until there is enough dry weight. This is called a bulk sample and should be indicated on the envelope (e.g. count the nr of leaves).\nWhen collecting the leaf, include petiole and rachis (Figure 5.3 (b)). Pat the leaf dry with a tissue if there is visible water on the surface. Add a sticker to enter the information and a barcode sticker to the envelope (Figure 5.6 and Figure 5.6 (a)). Attach the sticker with a stapler.\n\n\n\n\n\nLabel with information\n\n\n\n\n\n\n(a) Barcode with unique leaf ID\n\n\n\n\nFigure 5.6: Sticker and barcode example.\n\n\n\n\n\n\n\n\nHow IDs and barcodes are generated\n\n\n\nFor more information on how the unique leaf IDs and barcode stickers are generated, see the readme file in this GitHub repo.\n\n\nFill in all information between ‘collection day’ and ‘Bulk nr/length’. Carefully, check Table 5.1 for the different options for the information on the label.\nPlace the leaf inside the envelope and put the envelope in a box for the next step. Make sure the envelopes are kept wet at all times.\n\n\nTable 5.1: Information to write on the envelope.\n\n\n\n\n\n\nVariable\nWhat to fill in\n\n\n\n\n\nThree-D\nINCLINE\n\n\nCollection day\nadd day, e.g. 24\nadd day, e.g. 24\n\n\nSite\nVik, Joa, Lia\nUlv, Lav, Gud, Skj\n\n\nElevation\n469, 920, 1290\nx, x, x, x\n\n\nGenus\nBistorta\n\n\n\nSpecies\nvivipara\n\n\n\nProject\nTrait = 3D, Sean = group 3, Drone = group 5\nTrait = I\n\n\nExperiment\nambient/warm, N1-10, C/M/I/N\ncontrol or OTC\n\n\nPlotID\nturfID, e.g. 2 W5M 125\n???\n\n\nIndividual/leaf nr.\n1-5\n1-10/1-5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning for special cases.\n\n\n\nCarefully check the rules for how to measure special plant species plants.\n\n\nFestuca sp have tightly rolled leaves and they should not be unfolded for the scanning.\n\nHyperzia sp and Lycopodium sp have tiny leaves. Choose a 10cm piece of stem and scrape of all the leaves from this part for scanning. Make sure the leaves do not overlap"
  },
  {
    "objectID": "5_trait_collection.html#wet-mass",
    "href": "5_trait_collection.html#wet-mass",
    "title": "\n5  The trait wheel\n",
    "section": "\n5.3 Wet mass",
    "text": "5.3 Wet mass\nMake sure the envelopes are still kept wet. Weigh each leaf and write the weight on the envelope, including all digits."
  },
  {
    "objectID": "5_trait_collection.html#measure-leaf-area-using-a-scanner",
    "href": "5_trait_collection.html#measure-leaf-area-using-a-scanner",
    "title": "\n5  The trait wheel\n",
    "section": "\n5.4 Measure leaf area using a scanner",
    "text": "5.4 Measure leaf area using a scanner\nFor scanning the leaves we are using a Rasperry Pi setup. The Pis control the scanners, ensuring consistency, and validate file names.\n\n\n\n\n\n\nRaspberry Pi setup\n\n\n\nFor more information on the Raspberry Pi setup, see the readme file in this GitHub repo.\n\n\nStart setup\nConnect your Laptop to the Raspberry PI. Connect the PI with an ethernet cable to your laptop. Connect the scanner to the PI. Connect the barcode scanner to the PI. Connect the power cable to the PI.\nWait for one minute.\nOpen VNC Viewer and type in the IP address corresponding to your PI (Table 5.2).\n\n\nTable 5.2: IP addresses of the PIs.\n\nPI\nIP address\n\n\n\nobi wan\n169.254.109.180\n\n\nyoda\n169.254.160.193\n\n\nbb-8\n169.254.54.188\n\n\nc-3po\n169.254.247.22\n\n\ndarth vader\n169.254.178.187\n\n\nr2-d2\n169.254.231.2\n\n\ndeathstar\n169.254.152.80\n\n\nmr. spock\n169.254.204.207\n\n\n\n\n\n\nconnect with phone\n192.168.42.42\n\n\n\n\nThe spare SD card has the same IP address as the pi it is in!\nYou will be asked to type in a username and password:\nUsername: pi\nPassword: pftc\nResize the screen for convenience.\nScan leaves\nMake sure the envelopes are still kept wet. Check that the scanner is clean and free of debris. Clean if needed. Place wet leaf face-down on the scanner. Make sure the leaf is not folded and that leaflets do not overlap. The leaf should not go all the way to the edge of the scanner, because each picture will be cropped. You can cut the leaf into several pieces if needed, but please make a remark if you do so (e.g. cut 3). You can tape the leaf to the scanner using transparent tape if it folds (e.g. grasses).\nFor graminoids (grasses, sedges, rushes): Spread graminoid blades flat and tape to scanner if necessary to hold them in place. Exclude the leaf sheath (Figure 5.7).\n\n\n\n\nFigure 5.7: Leaf blade and sheats for a graminoid\n\n\n\n\n\n\n\n\n\n\nSpecial cases for scanning\n\n\n\nCarefully check the rules for how to scan special plant species plants.\n\n\nFestuca sp have tightly rolled leaves and they should not be unfolded for the scanning.\n\n\n\nScan, save and check the leaf\nClick on the leaf icon (only needs to be done the first time). This will start the scanning program. If the scanner cannot be found, wait for a bit and try again. Check the settings of the scanner. These should be set as standard, so please do not touch them if not needed.\nThe scanner settings should be:\n\nsaved in Desktop/leaf_scans/\nType: JPEG\nColour\nFull colour range\n300dpi\n\nFrom here, do this for each leaf:\nPress scan (at the bottom of the window) and wait a couple of seconds until the scanning process is done. A new window will pop up. Adjust the zoom to see the full picture (about 25%). Check the quality of the scan (has the whole leaf been scanned, is the leaf not upside down, dirt on the scan, etc.). If the scan is not ok, adjust the leaf and repeat the scan.\nWhen the scan is ok, click on the green save button to save the scan. Make sure the cursor marks the filename (should be default). Scan the barcode on the envelope of the leaf that has been scanned with the barcode reader. The filename should be something like: AAA4667 (3 letters and 4 digits). When the scan is saved, the PI will check that the LeafID and that the scanning settings (dpi, etc.) are correct. If anything is wrong, it will open a window with an error message (Figure 5.8). Please read it!\n\n\n\n\nFigure 5.8: Error message on the PI.\n\n\n\n\nClick anywhere in the window and it will disappear. Save the scan with the correct name or scan the leaf again if the settings are wrong. MAKE SURE TO DELETE THE SCAN WITH THE WRONG NAME OR SETTING. Tick the box on the envelope that the leaf has been scanned. Make sure the scanned leaves are kept wet and move them to the next station.\nFrom time to time:\nWhen you are finished with scanning click on the yoda icon. This will check the leafIDs. If anything is wrong it will open a window and indicate which scans are wrong. Make sure to fix all the wrong leafIDs and not to have duplicate scans before leaving the station.\nBefore leaving the station:\nCopy all the scans onto a stick and deposit them on the PFTC6 hard drive. Shut down the pi. Wait 1 min to disconnect it from the power."
  },
  {
    "objectID": "5_trait_collection.html#leaf-thickness",
    "href": "5_trait_collection.html#leaf-thickness",
    "title": "\n5  The trait wheel\n",
    "section": "\n5.5 Leaf thickness",
    "text": "5.5 Leaf thickness\nMake sure the envelopes are still kept wet. Thickness varies over the surface of the leaf; generally, the leaf is thickest at the midrib, primary veins, margins, and leaf base. Avoid the midrib when measuring leaf thickness, unless the leaf is too small (Figure 5.9). Take three thickness measurements at several points on the lamina and note them on the envelope. From this, we will later calculate the average thickness. If the leaf is small, it may not be possible to take three unique measurements.\n\n\n\n\nFigure 5.9: Showing where leaf thickness can be measured and where not, e.g. the midrib.\n\n\n\n\nFerns Make sure you do not measure thickness on the spore-producing sori on the backside of ferns."
  },
  {
    "objectID": "5_trait_collection.html#data-entry",
    "href": "5_trait_collection.html#data-entry",
    "title": "\n5  The trait wheel\n",
    "section": "\n5.6 Data entry",
    "text": "5.6 Data entry\nIdeally the envelope should still be kept wet until this point in case any information is missing. Go to this google sheet. Connect a barcode scanner to your computer. Scan the barcode of an envelope. Make sure that your cursor is in the right cell (ID). Enter all the data that is on the envelope (for details see Table 5.1). If any of the information is missing, check if it has been forgotten. Has the leaf been scanned, one thickness measurement is missing, etc. If anything is missing bring the leaf back to the according station in the trait wheel. When finished, check the box on the envelope that the data has been entered. Take a picture of the envelope. Bring the envelop to the drying station."
  },
  {
    "objectID": "5_trait_collection.html#dry-mass",
    "href": "5_trait_collection.html#dry-mass",
    "title": "\n5  The trait wheel\n",
    "section": "\n5.7 Dry mass",
    "text": "5.7 Dry mass\nThe leaves will be dried in the oven for 72 hours at 65°C."
  },
  {
    "objectID": "5_trait_collection.html#data-checking",
    "href": "5_trait_collection.html#data-checking",
    "title": "\n5  The trait wheel\n",
    "section": "\n5.8 Data checking",
    "text": "5.8 Data checking\nCheck scans Find the folder with the leave scans on google drive: link Open each scan and check the follwing criterias:\n\nIs the whole leaf on the scan?\nHas nothing else been scanned (cable, paper,…)?\nIs there dirty the scan?\nHas the leaf been cut? If yes, is it indicated on the envelope?\nHow many leaves are on the scan? If > 1 is there a remark on the envelope?\n\nCheck data Open RStudio project and the R script (…). Download the trait data from google sheet. Check if the leafID is valid. Check if the variables have valid entries. E.g. correct day, site name, elevations, etc. Check if missing values can be found on the envelope or retrieved from the data. Plot the data to check if traits have unrealistic values. For more details see section data curation."
  },
  {
    "objectID": "5_trait_collection.html#data-documentation",
    "href": "5_trait_collection.html#data-documentation",
    "title": "\n5  The trait wheel\n",
    "section": "\n5.9 Data documentation",
    "text": "5.9 Data documentation\n\ndescription of the method of data collection\ndata dictionary for each data set"
  },
  {
    "objectID": "5_trait_collection.html#references",
    "href": "5_trait_collection.html#references",
    "title": "\n5  The trait wheel\n",
    "section": "\n5.10 References",
    "text": "5.10 References\n\n\n\n\nPérez-Harguindeguy, N, S Dı́az, E Garnier, S Lavorel, H Poorter, P Jaureguiberry, M S Bret-Harte, et al. 2013. “New Handbook for Standardised Measurement of Plant Functional Traits Worldwide.” Australian Journal of Botany."
  },
  {
    "objectID": "6_pftc_data.html",
    "href": "6_pftc_data.html",
    "title": "6  Working with PFTC data",
    "section": "",
    "text": "The cleaned datasets from the PFTC courses are stored on an OSF repository (short repo). Each course has it’s own data repo. The following table shows the link to each of the repos.\n\n\nCourse\nCountry\nData repo\n\n\n\nPFTC1 and 2\nChina\nChina repo\n\n\nPFTC3 and 5\nPeru\nPeru repo\n\n\nPFTC4\nSvalbard\nSvalbard repo\n\n\nPFTC6\nNorway\n3D repo\n\n\n\n\nIncline repo"
  },
  {
    "objectID": "6_pftc_data.html#data-paper-and-data-documentation",
    "href": "6_pftc_data.html#data-paper-and-data-documentation",
    "title": "6  Working with PFTC data",
    "section": "\n6.2 Data paper and data documentation",
    "text": "6.2 Data paper and data documentation\nThe site, experiment, data collection methods and data dictionaries for each course are described in data papers. Some information can also be found on the GitHub readme file.\n\n\nCourse\nCountry\nData paper/documentation\n\n\n\nPFTC1 and 2\nChina\nData paper\n\n\nPFTC3 and 5\nPeru\nGitHub readme\n\n\nPFTC4\nSvalbard\nGitHub readme\n\n\nPFTC6\nNorway\n3D: GitHub readme\n\n\nPFTC6\nNorway\nIncline: GitHub readme\n\n\n\nRead the papers and documentation carefully before using the data."
  },
  {
    "objectID": "6_pftc_data.html#data-usage-and-citation",
    "href": "6_pftc_data.html#data-usage-and-citation",
    "title": "6  Working with PFTC data",
    "section": "\n6.3 Data usage and citation",
    "text": "6.3 Data usage and citation\nThe data are available for use and teaching purposes under a CC-BY licence. We suggest to cite the data paper if available, or alternatively the OSF repo. We appreciate being contacted for advice or collaboration, if relevant, by users of these data. In cases where our data make up >10% of the data used in a downstream publication, we suggest contacting us for our contribution and collaboration."
  },
  {
    "objectID": "6_pftc_data.html#download-pftc-data",
    "href": "6_pftc_data.html#download-pftc-data",
    "title": "6  Working with PFTC data",
    "section": "\n6.4 Download PFTC data",
    "text": "6.4 Download PFTC data\nWe have created a R package to download the data, called dataDownloader. If you are using the package for the first time you need to install the package using the command below. If you have used it before, just run the second line of code to load the package.\n\ndevtools::install_github(\"Between-the-Fjords/dataDownloader\")\nlibrary(dataDownloader)\n\nNow you can download the all the files you need. Let’s download the community data and the trait from the Svalbard course.\n\nget_file(node = \"smbqh\",\n         file = \"PFTC4_Svalbard_2018_Gradient_Traits.csv\",\n         path = \"webpage/data\",\n         remote_path = \"Traits\")\n                       \nget_file(node = \"smbqh\",\n         file = \"PFTC4_Svalbard_2018_Community_Gradient.csv\",\n         path = \"webpage/data\",\n         remote_path = \"Community\")\n\nExercise\nNow it is your turn. Copy the code into your console and download the data."
  },
  {
    "objectID": "7_biostats.html",
    "href": "7_biostats.html",
    "title": "\n7  Introduction to R, RStudio and GitHub\n",
    "section": "",
    "text": "The BioSTATS books are an extensive resource for getting started with R and RStudio, working in R, coding, using Rmarkdown, git and GitHub and creating an R package."
  },
  {
    "objectID": "8_reproducible.html",
    "href": "8_reproducible.html",
    "title": "\n8  Reproducible workflows\n",
    "section": "",
    "text": "Reproduciblitiy …"
  },
  {
    "objectID": "9_data_curation.html",
    "href": "9_data_curation.html",
    "title": "\n9  Data curation\n",
    "section": "",
    "text": "Data curation, transformation and cleaning are the first steps after digitizing the data. Each dataset has to be checked for several types of errors and corrected if possible. Some of the checking and correction can be done automatically, but other things have to be done manually. And sometimes we need play detectives to find the problems and the right way to correct errors. This tutorial shows the main steps for how to check a dataset and make corrections.\nFor this tutorial we will be working with the trait dataset from Svalbard. See chapter Chapter 6 for how to access the data and information about the study, experiment and datasets."
  },
  {
    "objectID": "9_data_curation.html#useful-packages",
    "href": "9_data_curation.html#useful-packages",
    "title": "\n9  Data curation\n",
    "section": "\n9.1 Useful packages",
    "text": "9.1 Useful packages\nThere are a couple of R packages that are useful for data curation. First, tidyverse is a collection of R packages used for basic data manipulation and analysis. We will also use lubridate, which helps with data and time formats.\nIf you have never used the packages you need to install it first using the function install.packages(\"tidyverse\"). Otherwise, you can just load the packages.\n\nlibrary(tidyverse)\nlibrary(lubridate)\n\nAnother useful package for data curation is tidylog, which is built on the dplyr and tidyr packages and provides useful information about the functions used.\nTidylog will for example tell you how many rows have been removed and are remaining when using the filter() function or how many rows match when using a join function. The information is always indicated in absolute numbers and percentage. This additional information is very useful to check if the right observations have been removed or manipulated, because mistakes are easily done.\nLet’s install and/or load tidylog.\n\nlibrary(tidylog)\n\nNote, that once tidylog is loaded it will automatically prioritize the tidylog function before the dplyr and tidyr functions. You actively have to choose if you do not want to use the tidylog version by using this notation: dplyr::filter().\nSome data checking has to be done by hand and detecitve work, other things can be done more automatically. There are a few packages that can help with some of this work and for this tutorial we will use the validate package.\n\n#install.packages(\"validate\")\nlibrary(validate)"
  },
  {
    "objectID": "9_data_curation.html#import-data",
    "href": "9_data_curation.html#import-data",
    "title": "\n9  Data curation\n",
    "section": "\n9.2 Import data",
    "text": "9.2 Import data\nThe first step is to import the dataset to R. The data is stored as a csv file and we can use the function read_csv() to import that data. If your data has another format or you are new to importing data, have a look at this page.\nGive the dataset a name using a backwards pointing arrow: <- The name should indicate that this is the raw data.\n\nraw_traits <- read_csv(\"data/PFTC4_Svalbard_2018_Gradient_Traits.csv\")\n#> Rows: 11345 Columns: 15\n#> ── Column specification ────────────────────────────────────────────────────────\n#> Delimiter: \",\"\n#> chr  (7): Project, Gradient, PlotID, ID, Functional_group, Taxon, Trait\n#> dbl  (7): Year, Site, Individual_nr, Value, Elevation_m, Latitude_N, Longitu...\n#> date (1): Date\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nThe dataset has 11345 rows and 15 columns and a number of numeric, character and date variables. It contains measurements of 14 traits from two elevational gradients on Svalbard. The traits were measured on individual plants from 21 different graminoid and forb species. For more information about the sites, traits and measurements see here.\nSome manipulation\nLet us introduce some errors to the dataset.\nThe code to do this is hidden. But if you want to replicate the code to introduce errors you can find the code from line 116."
  },
  {
    "objectID": "9_data_curation.html#view-dataset",
    "href": "9_data_curation.html#view-dataset",
    "title": "\n9  Data curation\n",
    "section": "\n9.3 View dataset",
    "text": "9.3 View dataset\nFirst, we want to have a look at the dataset. By typing raw_traits in the console it will display the first rows and columns of the dataset. Note that the last column and many rows are not shown.\n\nraw_traits\n#> # A tibble: 10,298 × 10\n#>    Date       Gradient  Site PlotID Individual_nr ID      Taxon    Trait   Value\n#>    <chr>      <chr>    <dbl> <chr>          <dbl> <chr>   <chr>    <chr>   <dbl>\n#>  1 2018-07-20 B            5 D                  1 AIB1395 saxifra… Plan… 6.5 e+0\n#>  2 2018-07-20 B            5 D                  1 AIB1395 saxifra… Wet_… 2.92e-2\n#>  3 2018-07-20 B            5 D                  1 AIB1395 saxifra… Dry_… 4   e-3\n#>  4 2018-07-20 B            5 D                  1 AIB1395 saxifra… Leaf… 5.66e-1\n#>  5 2018-07-20 B            5 D                  1 AIB1395 saxifra… Leaf… 6.75e-1\n#>  6 2018-07-20 B            5 D                  1 AIB1395 saxifra… SLA_… 1.69e+2\n#>  7 2018-07-20 B            5 D                  1 AIB1395 saxifra… LDMC  1.37e-1\n#>  8 2018-07-20 B            5 D                  1 AIB1395 saxifra… C_pe… 3.89e+1\n#>  9 2018-07-20 B            5 D                  1 AIB1395 saxifra… N_pe… 1.14e+0\n#> 10 2018-07-20 B            5 D                  1 AIB1395 saxifra… CN_r… 3.41e+1\n#> # … with 10,288 more rows, and 1 more variable: Elevation_m <dbl>\n\nAt the top you can see that the dataset has 10298 observations and 10 columns. These numbers give you a first indication if you have imported the right dataset, and if all observations and columns are there."
  },
  {
    "objectID": "9_data_curation.html#check-variable-type",
    "href": "9_data_curation.html#check-variable-type",
    "title": "\n9  Data curation\n",
    "section": "\n9.4 Check variable type",
    "text": "9.4 Check variable type\nOne of the things we want to do is checking if all the variables in the dataset have the right type. For each variable the output above indicates the data type just below the variable name. The most common types are dbl (numeric or integer), chr (character), or date (date).\nIf you are unfamiliar with data types see here.\nThe first variable Date is a character, which does not seem to be correct. This means that one or several observations in this column are not dates. Since we do not expect to have very many dates (the data was collected during a few days), we can check all the different values in Date. For this we use the function distinct() on the variable Date.\n\nraw_traits |> \n  distinct(Date)\n#> distinct: removed 10,291 rows (>99%), 7 rows remaining\n#> # A tibble: 7 × 1\n#>   Date      \n#>   <chr>     \n#> 1 2018-07-20\n#> 2 2018-07-18\n#> 3 2018-07-21\n#> 4 2018-07-19\n#> 5 2018-07-17\n#> 6 18        \n#> 7 <NA>\n\nWe see that there are 6 distinct dates in the variable Date. One of the dates is “18”, which is not a correct date format and turned the variable into a character. Note the additional information from the tidylog package on the distinct() function, which shows the number of rows removed and remaining.\nThe next step is to check which observastion(s) have the wrong date. For this we can use the function filter() to extract all observations with the date 18. We can pipe this to View(), which will display the whole table in a separate window. Note that for this tutorial, we use a different way of displaying the output\n\nraw_traits |> \n  filter(Date == \"18\") |> \n  View()\n\n\n#> filter: removed 10,291 rows (>99%), 7 rows remaining\n#> # A tibble: 7 × 10\n#>   Date  Gradient  Site PlotID Individual_nr ID      Taxon         Trait    Value\n#>   <chr> <chr>    <dbl> <chr>          <dbl> <chr>   <chr>         <chr>    <dbl>\n#> 1 18    C            1 A                  3 AMO3822 salix polaris Plant… 1.1 e+0\n#> 2 18    C            1 A                  3 AMO3822 salix polaris Wet_M… 5.76e-3\n#> 3 18    C            1 A                  3 AMO3822 salix polaris Dry_M… 2   e-3\n#> 4 18    C            1 A                  3 AMO3822 salix polaris Leaf_… 1.88e-1\n#> 5 18    C            1 A                  3 AMO3822 salix polaris Leaf_… 2.84e-1\n#> 6 18    C            1 A                  3 AMO3822 salix polaris SLA_c… 1.42e+2\n#> 7 18    C            1 A                  3 AMO3822 salix polaris LDMC   3.47e-1\n#> # … with 1 more variable: Elevation_m <dbl>\n\nWe can see that a single observation (with multiple traits) has the wrong date. There is no way that we would remember on what day this leaf was collected (remember the dataset has > 10000 leaves!). We have to start playing detectives now. The only way to find the right date for these observations is to check the raw data (leaves), notes or photos. It is therefore important to keep all the data entry sheets (paper version), take a photo of them and make tidy notes during field work. This is the only way to fix many of the problems.\nLuckily, we took pictures from all envelopes of the leaves. The date on the envelope is 18 July 2018 (Figure 9.1), and it seems that there has been a typo.\n\n\n\n\nFigure 9.1: The envelope of the leaf with the wrong date.\n\n\n\n\nLet’s replace the wrong date and give the variable Date the right class.\nFor this we will use the function mutate() which adds or manipulates a column. Inside the mutate we will use the if_else() function to replace the date for a specific ID. This function is useful for a single condition. However for multiple statements (many if else conditions), we recommend to use the case_when() function (see below). To change the class, we use the ymd() function from the lubridate package. Note that we now have to assign the table to a new or the same name to make the change permanent.\n\nraw_traits <- raw_traits |> \n  mutate(Date = if_else(ID == \"AMO3822\", \"2018-07-18\", Date)) |> \n  mutate(Date = ymd(Date))\n#> mutate: changed 7 values (<1%) of 'Date' (0 new NA)\n#> mutate: converted 'Date' from character to Date (0 new NA)\n\nAn important step and good practice when cleaning data is to check that the right correction has been done. Here is where the tidylog package comes in handy. It shows that for 7 observation Date has been changed. This matches with the number of observations that had a wrong date.\nTo be absolutely sure we can look at the specific leaf (ID == “AMO3822”) and see if the date is now corrected. Another way would be to run the distinct(Date) function again.\n\nraw_traits |> \n  filter(ID == \"AMO3822\") |> \n  select(Date)\n#> filter: removed 10,291 rows (>99%), 7 rows remaining\n#> select: dropped 9 variables (Gradient, Site, PlotID, Individual_nr, ID, …)\n#> # A tibble: 7 × 1\n#>   Date      \n#>   <date>    \n#> 1 2018-07-18\n#> 2 2018-07-18\n#> 3 2018-07-18\n#> 4 2018-07-18\n#> 5 2018-07-18\n#> 6 2018-07-18\n#> 7 2018-07-18\n\nThe date has been fixed.\n\n\n\n\n\n\nExercise 1\n\n\n\nNow it is your turn. Check if the data type for the variable Date is now correct.\nHint\n\ntype raw_traits to look at the whole dataset where the datatype of each variable is indicated\nuse class(raw_traits$Date) which will tell you directly what type of class the variable has\nuse map(raw_traits, class) to get the class of all variable in the dataframe\n\n\n\nUsing the validate package, we can check all the variables at once. The validate package is based on making some rules that need checking and then applying those rules to a dataset. The rules can be reused and applied to any dataset.\nLet’s make some rules about data types using the validator() function:\n\n# rules\nrules <- validator(\n  \n  # check variable types\n  is.character(Gradient),\n  is.character(PlotID),\n  is.character(ID),\n  is.character(Taxon),\n  is.character(Trait),\n\n  is.numeric(Site),\n  is.numeric(Individual_nr),\n  is.numeric(Value),\n  is.numeric(Elevation_m),\n  \n  is.Date(Date))\n\nNow the rules can be applied to the dataset using the confront() function.\n\nout <- confront(raw_traits, rules)\nsummary(out)\n  \n\nThe summary function gives an overview of each rule and how many passes and fails there are. It looks like all the rules are passed."
  },
  {
    "objectID": "9_data_curation.html#check-for-duplicates",
    "href": "9_data_curation.html#check-for-duplicates",
    "title": "\n9  Data curation\n",
    "section": "\n9.5 Check for duplicates",
    "text": "9.5 Check for duplicates\nAnother common problem is duplicate observations. This can happen when data is entered twice. The why to find duplicates is to check that the combination of variables are unique. In our dataset, we expect that Date, Gradient, Site, PlotID, Individual_nr, ID, Taxon and Trait should be unique, and only occurring once.\nTo check this, we can use the rule is_unique().\nNote that Value was not included in the is_unique(). This was done intentionally, because a common mistake is to have a duplicate, but with a different value. This is either because one of the variables is wrong, e.g. it has the wrong Site and therefore appears to be a duplicate. Alternatively, the leaf could have been measured twice by accident, which would likely give two slightly different values. When getting a duplicate, these different options for why there is a duplicate have to be considered and carefully checked in the raw data.\n\nrules <- validator(\n  \n  # check variable types\n  is_unique(Date, Gradient, Site, PlotID, Individual_nr, ID, Taxon, Trait))\n\nout <- confront(raw_traits, rules)\nsummary(out)\n#>   name items passes fails nNA error warning\n#> 1   V1 10298  10296     2   0 FALSE   FALSE\n#>                                                                 expression\n#> 1 is_unique(Date, Gradient, Site, PlotID, Individual_nr, ID, Taxon, Trait)\n\nTwo observations fail the rules and are not unique.\nThe violate package can visualise the number of passes and fails, which can be useful. For this, use the plot function.\n\nplot(out)\n\n\n\n\nTo fix the problem, we need to know which observation is a duplicate. Here, we can use the function violating() for the data and the results and it will show the duplicate rows.\n\nviolating(raw_traits, out)\n#> # A tibble: 2 × 10\n#>   Date       Gradient  Site PlotID Individual_nr ID      Taxon     Trait   Value\n#>   <date>     <chr>    <dbl> <chr>          <dbl> <chr>   <chr>     <chr>   <dbl>\n#> 1 2018-07-20 B            3 C                  3 BEK3638 salix po… Dry_… 0.00275\n#> 2 2018-07-20 B            3 C                  3 BEK3638 salix po… Dry_… 0.00275\n#> # … with 1 more variable: Elevation_m <dbl>\n\nWe get two exact duplicates, where even Value is the same. We can therefore assume that the leaf has only been measured once, but the data has been entered twice.\nTo fix the problem, we want to remove one of the duplicates. We group by the variables we expect to be unique and use distinct() with the argument .keep_all = TRUE to remove the duplicates.\n\nraw_traits <- raw_traits |> \n  group_by(Date, Gradient, Site, PlotID, Individual_nr, ID, Taxon, Trait) |> \n  distinct(.keep_all = TRUE) |> \n  ungroup()\n#> group_by: 8 grouping variables (Date, Gradient, Site, PlotID, Individual_nr, …)\n#> distinct (grouped): removed one row (<1%), 10,297 rows remaining\n#> ungroup: no grouping variables\n\nTidylog shows again what happens and how many rows have been removed. There are 8 grouping variables and as expected, one row is removed, which is the duplicated row.\nWe can also run the code from above again to check if the duplicate is gone."
  },
  {
    "objectID": "9_data_curation.html#check-for-missing-data",
    "href": "9_data_curation.html#check-for-missing-data",
    "title": "\n9  Data curation\n",
    "section": "\n9.6 Check for missing data",
    "text": "9.6 Check for missing data\nA common problem in a dataset are missing data. There are many reasons for having missing data. For now, we want to find out if we have any NAs in the dataset and if yes where and how many.\nA quick way to get an overview of all NAs in the dataset is to select for any NAs in the dataset and summarise how many NAs there are.\n\nraw_traits %>% \n  select(where( ~any(is.na(.)))) %>% \n  summarise(across(everything(), ~sum(is.na(.))))\n#> select: dropped 8 variables (Gradient, Site, PlotID, Individual_nr, ID, …)\n#> summarise: now one row and 2 columns, ungrouped\n#> # A tibble: 1 × 2\n#>    Date Value\n#>   <int> <int>\n#> 1     7     3\n\nWe can see that Date and Value have NAs. It is not always a problem to have missing data. In this case, Date is not a crucial variable, and we know the data was collected during a few days in July 2018. We could just impute one of these dates. But for now, let’s focus on the NAs in Value.\nOnce the missing values are detected one has to decide if the missing data can be recovered, or if the missing values should be removed from the dataset. After checking all the raw data and notes, we cannot find the Values from these observations and have to conclude that the data is useless. So, we want to remove them. For this we will use the function drop_na() on the variable Value.\n\nraw_traits <- raw_traits |> \n  drop_na(Value)\n#> drop_na: removed 3 rows (<1%), 10,294 rows remaining\n\nThis operation has removed 3 rows, which is the number of NA’s in this variable.\nMissing data can also be found with the validator package, but it’s a bit tedious to write one rule (is.na() or !is.na()) for each variable. In some cases it might be useful to use, because it let’s you use functions like any() or all() to"
  },
  {
    "objectID": "9_data_curation.html#check-specific-values",
    "href": "9_data_curation.html#check-specific-values",
    "title": "\n9  Data curation\n",
    "section": "\n9.7 Check specific values",
    "text": "9.7 Check specific values\nOne of the things we want to check is if the unique leaf IDs are valid. For this, we need to get a list of these IDs, using the get_PFTC_envelope_codes function from the PFTCFunctions package.\n\n#remotes::install_github(\"Plant-Functional-Trait-Course/PFTCFunctions\")\n\nlibrary(\"PFTCFunctions\")\n\nleaf_ID <- get_PFTC_envelope_codes(seed = 32)"
  },
  {
    "objectID": "9_data_curation.html#check-taxonomy",
    "href": "9_data_curation.html#check-taxonomy",
    "title": "\n9  Data curation\n",
    "section": "\n9.8 Check taxonomy",
    "text": "9.8 Check taxonomy\nA common problem is inconsistencies within variables. In this dataset such a variable is Taxon. It is very common to make mistakes and typos with Latin species names during data entry.\nLet’s look at all unique species names using distinct() and sort them by Taxon using arrange(). This is a good way to see small typos in the species names.\n\nraw_traits |> \n  distinct(Taxon) |> \n  arrange(Taxon) |> \n  print(n = Inf)\n#> distinct: removed 10,259 rows (>99%), 35 rows remaining\n#> # A tibble: 35 × 1\n#>    Taxon                   \n#>    <chr>                   \n#>  1 alopecurus ovatus       \n#>  2 bistorta vivipara       \n#>  3 calalmagrostis neglecta \n#>  4 calamagrostis neglecta  \n#>  5 cassiope tetragona      \n#>  6 cerastium arcticum      \n#>  7 draba arctica           \n#>  8 draba oxycarpa          \n#>  9 dryas octopetala        \n#> 10 equisetum arvense       \n#> 11 equisetum scirpoides    \n#> 12 festuca rubra           \n#> 13 festuca viviparoidea    \n#> 14 luzula confusa          \n#> 15 luzula nivalis          \n#> 16 micranthes hieraciifolia\n#> 17 micranthes nivalis      \n#> 18 oxiria digyna           \n#> 19 oxyra digyna            \n#> 20 oxyria digina           \n#> 21 oxyria digyna           \n#> 22 pedicularis hirsuta     \n#> 23 poa alpina              \n#> 24 poa arctica             \n#> 25 poa pratensis           \n#> 26 potentilla hyparctica   \n#> 27 ranunculus sulphureus   \n#> 28 salix polaris           \n#> 29 saxifraga cernua        \n#> 30 saxifraga cespitosa     \n#> 31 saxifraga hirculus      \n#> 32 saxifraga oppositifolia \n#> 33 silene acaulis          \n#> 34 stellaria longipes      \n#> 35 trisetum spicatum\n\nThere are four different versions for oxyra digyna and two for calamagrostis neglecta. Obviously, some typos where made when entering the data.\n\n9.8.1 Use case_when()\nBecause we have to change multiple species names, we will use case_when(), which allows for multiple conditions.\n\nraw_traits <- raw_traits |> \n  mutate(Taxon = case_when(Taxon %in% c(\"oxiria digyna\", \n                                        \"oxyria digina\", \n                                        \"oxyra digyna\") ~ \"oxyria digyna\",\n                           Taxon == \"calalmagrostis neglecta\" ~ \"calamagrostis neglecta\",\n                           TRUE ~ Taxon))\n#> mutate: changed 38 values (<1%) of 'Taxon' (0 new NA)\n\n\n9.8.2 Use taxon dictionary\nAn alternative to using case_when() to fix the problem, would be to create a dictionary with bad and good species names.\nLet’s make a taxon dictionary.\n\ndictionary <- tibble(bad_name = c(\"oxiria digyna\", \n                                  \"oxyria digina\", \n                                  \"oxyra digyna\", \n                                  \"calalmagrostis neglecta\"),\n                     good_name = c(\"oxyria digyna\", \n                                   \"oxyria digyna\", \n                                   \"oxyria digyna\", \n                                   \"calamagrostis neglecta\"))\n\nNext, we need to join the dictionary to the dataset using the bad name column. And with coalesce with can replace the bad names with the good names.\n\nraw_traits <- raw_traits |> \n  left_join(dictionary, by = c(\"Taxon\" = \"bad_name\")) |> \n  mutate(Taxon = coalesce(good_name, Taxon))\n  \n\n\n9.8.3 Checking Taxon using TNRS\nIt is always advisable to check the taxonomy using a database such as TNRS, a Taxonomic Name Resolution Service.\nMaitner please add an example using TNRS."
  },
  {
    "objectID": "9_data_curation.html#visualise-data",
    "href": "9_data_curation.html#visualise-data",
    "title": "\n9  Data curation\n",
    "section": "\n9.9 Visualise data",
    "text": "9.9 Visualise data\nSome errors and problems in the data are difficult to detect by looking at the dataset. For example checking if the measurements are realistic is nearly impossible by going through a table with numbers. For this, visualising the data is much more effective.\n\n9.9.1 Histogram or density plot\nUsing histograms or density plots shows you the range of values in a variable. We are showing the density for each trait and colour the two different gradients.\n\nraw_traits |> \n  ggplot(aes(x = Value, fill = Gradient)) +\n  geom_density(alpha = 0.7) +\n  scale_fill_manual(values = c(\"green4\", \"grey\")) +\n  facet_wrap(~ Trait, scales = \"free\")\n\n\n\nFigure 9.2: Density distributions of all measured traits.\n\n\n\n\nNote that the size traits (plant height, leaf mass, area and thickness) have distributions with very long tails. This is common for size related variables and log transformation is common for such variables.\nAlso not that leaf area has a huge tail and goes up to almost 20’000 cm2. This is a leaf of almost 2 m2, which is impossible for a plant from Svalbard. This value needs to be checked, it could be a typo.\nLet’s log transform the size traits first.\n\n\nraw_traits <- raw_traits |> \n  mutate(Value_log = if_else(Trait %in% c(\n    \"Plant_Height_cm\",\n    \"Wet_Mass_g\",\n    \"Dry_Mass_g\",\n    \"Leaf_Area_cm2\",\n    \"Leaf_Thickness_mm\"), log(Value), Value),\n    Trait = recode(Trait,\n                   \"Plant_Height_cm\" = \"Plant_Height_cm_log\",\n                   \"Wet_Mass_g\" = \"Wet_Mass_g_log\",\n                   \"Dry_Mass_g\" = \"Dry_Mass_g_log\",\n                   \"Leaf_Area_cm2\" = \"Leaf_Area_cm2_log\",\n                   \"Leaf_Thickness_mm\" = \"Thickness_mm_log\"))\n#> Warning in log(Value): NaNs produced\n#> mutate: changed 5,214 values (51%) of 'Trait' (0 new NA)\n#>         new variable 'Value_log' (double) with 7,991 unique values and 0% NA\n\nAnd remake the density plot using the log-transformed values.\n\nraw_traits |> \n  ggplot(aes(x = Value_log, fill = Gradient)) +\n  geom_density(alpha = 0.7) +\n  scale_fill_manual(values = c(\"green4\", \"grey\")) +\n  facet_wrap(~ Trait, scales = \"free\")\n\n\n\nFigure 9.3: Density distributions of all measured traits.\n\n\n\n\nThe size traits do not have a long tail anymore.\nLet’s find the giant leaf. For this we can filter observations in the trait Leaf Area that have Value larger than 10.\n\nraw_traits |> \n  filter(Trait == \"Leaf_Area_cm2_log\",\n         Value > 10)\n#> filter: removed 10,293 rows (>99%), one row remaining\n#> # A tibble: 1 × 11\n#>   Date       Gradient  Site PlotID Individual_nr ID      Taxon       Trait Value\n#>   <date>     <chr>    <dbl> <chr>          <dbl> <chr>   <chr>       <chr> <dbl>\n#> 1 2018-07-18 C            5 C                  2 ANH3472 oxyria dig… Leaf… 17965\n#> # … with 2 more variables: Elevation_m <dbl>, Value_log <dbl>\n\nThis value for Leaf Area is impossible. We can again check the envelope of this leaf to find out if this was a typo. It turns out the comma was missed when typing in the data. Let’s fix the problem with a mutate and if_else() statement. Note that we have to fix the problem for the Value and Value_log column.\n\nraw_traits <- raw_traits |> \n  mutate(Value = if_else(ID == \"ANH3472\" & Trait == \"Leaf_Area_cm2_log\", 1.7965, Value),\n         Value_log = if_else(ID == \"ANH3472\" & Trait == \"Leaf_Area_cm2_log\", log(1.7965), Value_log))\n#> mutate: changed one value (<1%) of 'Value' (0 new NA)\n#>         changed one value (<1%) of 'Value_log' (0 new NA)\n\n\n9.9.2 Correlations\nAnother way to check the data is to plot variables against each other that should be correlated. In this dataset, we can plot dry mass against leaf area. We would expect a positive correlation between the two variables, where large leaves have a higher dry mass.\n\nraw_traits |>\n  select(-Value) |> \n  pivot_wider(names_from = Trait, values_from = Value_log) |>\n  ggplot(aes(x = Dry_Mass_g_log, y = Leaf_Area_cm2_log)) +\n  geom_point()\n#> select: dropped one variable (Value)\n#> pivot_wider: reorganized (Trait, Value_log) into (Plant_Height_cm_log, Wet_Mass_g_log, Dry_Mass_g_log, Thickness_mm_log, Leaf_Area_cm2_log, …) [was 10294x10, now 1074x22]\n#> Warning: Removed 39 rows containing missing values (geom_point).\n\n\n\nFigure 9.4: Correlation between leaf area and dry mass.\n\n\n\n\nWe see a good correlation between leaf area and dry mass. However, there is a cloud with observations that separate from the main data cloud.\n\n\n\n\n\n\nExercise 2\n\n\n\nWhat could the problem be?\nHint\n\nUse filter( to look at the observations that have a higher leaf area and mass and compare them to the rest of the data.\nUnits"
  },
  {
    "objectID": "10_community.html",
    "href": "10_community.html",
    "title": "\n10  Plant community composition data\n",
    "section": "",
    "text": "For this tutorial we will use the follwoing packages: tidyverse, vegan, ggvegan and broom."
  },
  {
    "objectID": "10_community.html#the-data",
    "href": "10_community.html#the-data",
    "title": "\n10  Plant community composition data\n",
    "section": "\n10.1 The data",
    "text": "10.1 The data\nWe will use the data from PFTC3 and 5 from Peru.\nTo download the data use this code:\n\n\nlibrary(dataDownloader)\n\nget_file(node = \"gs8u6\",\n         file = \"PFTC3-Puna-PFTC5_Peru_2018-2020_CommunityCover_clean.csv\",\n         path = \"clean_data\",\n         remote_path = \"community\")\n\nThis dataset contains percentage cover for 143 species along an elevational gradients in Peru. Three different treatments related to fire history: control (C), burnt (B) and newly burnt (NB)). For more information about the sites and measurements see here.\nTo read in the data use the code below. Note that we remove the treatment BB which has only a few observation from one site.\n\n\ncomm <- read_csv(\"data/PFTC3-Puna-PFTC5_Peru_2018-2020_CommunityCover_clean.csv\") |> \n  filter(treatment != \"BB\")\n#> Rows: 3665 Columns: 15\n#> ── Column specification ────────────────────────────────────────────────────────\n#> Delimiter: \",\"\n#> chr (8): season, month, site, treatment, family, functional_group, taxon, co...\n#> dbl (7): year, plot_id, cover, burn_year, elevation, latitude, longitude\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "10_community.html#diversity-indices",
    "href": "10_community.html#diversity-indices",
    "title": "\n10  Plant community composition data\n",
    "section": "\n10.2 Diversity indices",
    "text": "10.2 Diversity indices\nOne way to quantify the number of species species is to calculate diversity indices. Some common indices are species richness, diversity and evenness.\nTo calculate these indices we are using the vegan package.\n\n\n## Calculate responses\ndiversity_index <- comm  |> \n  group_by(year, month, site, elevation, treatment, plot_id)  |> \n  summarise(richness = n(),\n            diversity = diversity(cover),\n            evenness = diversity/log(richness))  |> \n  # make long dataframe\n  pivot_longer(cols = c(richness:evenness), names_to = \"index\", values_to = \"value\") |> \n  mutate(index = factor(index, levels = c(\"richness\", \"diversity\", \"evenness\")))\n#> `summarise()` has grouped output by 'year', 'month', 'site', 'elevation',\n#> 'treatment'. You can override using the `.groups` argument.\n\nWe can now test if these diversity indices are different between the sites along the elevational gradient and the treatments. We will use a simple linear model with diversity index as response and the interaction of elevation and treatment as predictor.\n\n\ndiversity_result <- diversity_index  |> \n  filter(treatment %in% c(\"C\", \"B\", \"NB\"))  |> \n  mutate(treatment = factor(treatment, levels = c(\"C\", \"B\", \"NB\")))  |> \n  group_by(index)  |> \n  nest(data = -c(index))  |> \n  mutate(model = map(data, ~lm(value ~ elevation * treatment, data = .x)),\n         result = map(model, tidy))  |> \n  unnest(result)\n\ndiversity_result |> \n  select(index, term:p.value) |> \n  kbl(digits = 2)\n\n\n\n\n index \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n  \n\n\n richness \n    (Intercept) \n    6.39 \n    5.11 \n    1.25 \n    0.21 \n  \n\n richness \n    elevation \n    0.00 \n    0.00 \n    2.25 \n    0.03 \n  \n\n richness \n    treatmentB \n    -15.81 \n    7.41 \n    -2.13 \n    0.03 \n  \n\n richness \n    treatmentNB \n    -105.11 \n    15.57 \n    -6.75 \n    0.00 \n  \n\n richness \n    elevation:treatmentB \n    0.00 \n    0.00 \n    2.30 \n    0.02 \n  \n\n richness \n    elevation:treatmentNB \n    0.03 \n    0.00 \n    6.44 \n    0.00 \n  \n\n diversity \n    (Intercept) \n    1.59 \n    0.49 \n    3.24 \n    0.00 \n  \n\n diversity \n    elevation \n    0.00 \n    0.00 \n    0.63 \n    0.53 \n  \n\n diversity \n    treatmentB \n    -0.59 \n    0.71 \n    -0.83 \n    0.41 \n  \n\n diversity \n    treatmentNB \n    -7.06 \n    1.49 \n    -4.72 \n    0.00 \n  \n\n diversity \n    elevation:treatmentB \n    0.00 \n    0.00 \n    0.86 \n    0.39 \n  \n\n diversity \n    elevation:treatmentNB \n    0.00 \n    0.00 \n    4.47 \n    0.00 \n  \n\n evenness \n    (Intercept) \n    0.70 \n    0.15 \n    4.80 \n    0.00 \n  \n\n evenness \n    elevation \n    0.00 \n    0.00 \n    -0.29 \n    0.77 \n  \n\n evenness \n    treatmentB \n    -0.06 \n    0.21 \n    -0.26 \n    0.79 \n  \n\n evenness \n    treatmentNB \n    -1.01 \n    0.45 \n    -2.27 \n    0.02 \n  \n\n evenness \n    elevation:treatmentB \n    0.00 \n    0.00 \n    0.24 \n    0.81 \n  \n\n evenness \n    elevation:treatmentNB \n    0.00 \n    0.00 \n    2.16 \n    0.03 \n  \n\n\n\n\nSpecies richness and diversity increase for the NB treatment, but not for the other treatments.\nCheck model assumptions\nTo check if the model assumptions are met, we will use the performance package.\n\n\nrichness <- diversity_index |> \n  filter(index == \"richness\")\n\nfit <- lm(value ~ elevation * treatment, data = richness)\n\nplot <- check_model(fit)\n\nplot\n\nDiversity change with elevation\nWe can plot the diversity indices across elevation.\n\n\ndiversity_index %>%\n  ggplot(aes(x = elevation, y = value, colour = treatment, fill = treatment)) +\n  geom_point(alpha = 0.4) +\n  geom_smooth(method = \"lm\", formula = \"y ~ x\", alpha = 0.2) +\n  scale_colour_manual(\"Treatment\", values = puna_treatment_colour$colour[1:3]) +\n  scale_fill_manual(\"Treatment\", values = puna_treatment_colour$colour[1:3]) +\n  labs(x = \"Elevation m a.s.l\", y = \"\") +\n  guides(linetype = \"none\",\n         fill = \"none\",\n         colour = guide_legend(override.aes = list(fill = NA))) +\n  facet_wrap( ~ index, scales = \"free_y\")\n\n\n\n\nDiscuss restuls…"
  },
  {
    "objectID": "10_community.html#multivariate-analysis---nmds-ordination",
    "href": "10_community.html#multivariate-analysis---nmds-ordination",
    "title": "\n10  Plant community composition data\n",
    "section": "\n10.3 Multivariate analysis - NMDS ordination",
    "text": "10.3 Multivariate analysis - NMDS ordination\nIn addition to univariate descriptor of communities such as diversity indices, we might be interested in the species composition along the elevational gradient.\nA common method is non-metric multidimensional scaling (NMDS). A NMDS collapses information from multiple dimensions to a few dimensions. This allows to visualise the data more easily.\nAn NMDS can be done using the metaMDS() function form the vegan package.\n\n\ncover_fat <- comm %>%\n  select(-family, -functional_group, -c(burn_year:course)) %>%\n  arrange(year, season, month) %>%\n  pivot_wider(names_from = \"taxon\", values_from = \"cover\", values_fill = 0) %>%\n  ungroup()\n\ncover_fat_spp <- cover_fat %>% select(-(year:plot_id))\n\nset.seed(32)\nNMDS <- metaMDS(cover_fat_spp,\n                noshare = TRUE,\n                try = 30,\n                trace = 0)\n\nfNMDS <- fortify(NMDS) %>%\n  filter(Score == \"sites\") %>%\n  bind_cols(cover_fat %>% select(year:plot_id))\n\nCheck stress… (explain what it is and why it is done)\n\n\n  NMDS_1 <-  metaMDS(cover_fat_spp, noshare = TRUE, try = 30, k = 1)\n  NMDS_2 <-  metaMDS(cover_fat_spp, noshare = TRUE, try = 30, k = 2)\n  NMDS_3 <-  metaMDS(cover_fat_spp, noshare = TRUE, try = 30, k = 3)\n  NMDS_4 <-  metaMDS(cover_fat_spp, noshare = TRUE, try = 30, k = 4)\n  NMDS_5 <-  metaMDS(cover_fat_spp, noshare = TRUE, try = 30, k = 5)\n  NMDS_6 <-  metaMDS(cover_fat_spp, noshare = TRUE, try = 30, k = 6)\n\ntibble(\n    stress = c(NMDS_1$stress, NMDS_2$stress, NMDS_3$stress, NMDS_4$stress, NMDS_5$stress, NMDS_6$stress),\n    dimensions = c(1:6)) %>%\n    ggplot(aes(x = dimensions, y = stress)) +\n    geom_point()\n\nDiscuss the results of the stress plot and what desicion is taken. How many dimesions are chosen.\nVisualise\nThe output of an ordination can be visualised.\n\n\nfortify(fNMDS) %>%\n  mutate(treatment = factor(treatment, levels = c(\"C\", \"B\", \"NB\")),\n         site = factor(site, levels = c(\"WAY\", \"ACJ\", \"PIL\", \"TRE\", \"QUE\", \"OCC\")),\n         season = if_else(season == \"dry_season\",\n                          \"Dry season\",\n                          \"Wet season\")) %>%\n  ggplot(aes(x = NMDS1, y = NMDS2, colour = site, shape = treatment)) +\n  geom_point() +\n  scale_colour_manual(\"Site\", values = puna_site_colour$colour) +\n  scale_shape_manual(\"Treatment\", values=c(16, 5, 6)) +\n  coord_equal() +\n  facet_wrap(~ season)\n\n\n\n\nDiscuss results."
  },
  {
    "objectID": "11_bootstrap.html",
    "href": "11_bootstrap.html",
    "title": "\n11  Bootstrapping trait data\n",
    "section": "",
    "text": "Figure 11.1: ?(caption)\nThis vignette explains how to use the traitstrap package (Telford et al). For more details on the methods see Maitner et al.\nFirst of all, relax and turn on some music. We have prepared the traitstrap playlist for you!"
  },
  {
    "objectID": "11_bootstrap.html#the-aim-of-traitstrap",
    "href": "11_bootstrap.html#the-aim-of-traitstrap",
    "title": "\n11  Bootstrapping trait data\n",
    "section": "\n11.1 The aim of traitstrap",
    "text": "11.1 The aim of traitstrap\nTrait distributions can be used to infer the importance of community assembly processes and the role of climate drivers in shaping species and community responses to climate change. Community ecology has typically focused on the mean, however the higher moments (variance, skewness, and kurtosis) of trait distributions can reveal information about the various processes shaping species diversity.\n\n\n\n\nTrue distribution, mean and higher moments.\n\n\n\n\nMeasuring trait distributions is often difficult and time-consuming as it requires information on measuring trait values of all individuals present. Sampling protocols often limit sampling to a non-representative subset of the community, or rely upon species-level average traits values calculated in other locations or across many locations.\nTraditionally the moments of trait distributions have been estimated using weighting approaches that rely on the average traits of species weighted by some measure of abundance within the community. Such community-weighted trait moments methods assume that a species’ trait expression can be adequately represented by the mean, ignoring intraspecific trait variation.\nTo more accurately estimate local trait distributions, trait sampling should thus occur both across multiple individuals within each species, and across multiple locations or experimental treatments across the extent of the study in order to capture both inter- and intra-specific variability.\n\n\n\n\nCommunity weighted mean versus bootstrapping method.\n\n\n\n\nTraitstrap is an R package to estimate the moments of community trait distributions using a bootstrapping approach. Further, this package uses a hierarchical sampling design, which allows accounting for incomplete trait collections, traits from different spatial or temporal levels (e.g. local traits vs. databases), taxonomic hierarchies (e.g., species vs genus) and experimental designs (e.g., multiple sites, or treated vs. control sampling units).\nThe package has three main functions:\n\n\ntrait imputation function which allows to account for intraspecific trait variation and hierarchical sampling design.\na resample method using bootstrapping (parametric or nonparametric method) to calculate community weighted mean and happy moments (variance, skewness and kurtosis).\na summary function that summarizes the trait moments and calculates confidence intervals.\n\nNote that for this tutorial we are calling the mean and the higher moments the happy moments :-)"
  },
  {
    "objectID": "11_bootstrap.html#the-data",
    "href": "11_bootstrap.html#the-data",
    "title": "\n11  Bootstrapping trait data\n",
    "section": "\n11.2 The data",
    "text": "11.2 The data\nFor this vignette we will use part of a vascular plant dataset from two sites near Longyearbyen on Svalbard. The data was collected during the Plant Functional Trait Course in 2018 and contains data on the plant community composition and functional traits. For more details see this GitHub repo\nNote that some of the species names have been adapted."
  },
  {
    "objectID": "11_bootstrap.html#organize-your-data",
    "href": "11_bootstrap.html#organize-your-data",
    "title": "\n11  Bootstrapping trait data\n",
    "section": "\n11.3 Organize your data",
    "text": "11.3 Organize your data\nTo run traitstrap two datasets are required:\n\none dataset with information on abundance (e.g. biomass, size, cover, etc.) of the community, which is used to weight species traits by abundance in the community.\none dataset with the traits for each species (or as many species and individuals you have data for) in your community.\n\nThe datasets need to be organized in a tidy and long format and certain columns (see below) are required, but the naming of these columns are up to the user.\nLet us have a look at these datasets in an example.\nThe community data should have information the abundance of species in the community. This dataset will be used to weigh the traits by species abundance. Note that abundance can also be cover, size, biomass, or something similar.\nIn this example the contains species names (e.g. Taxon), cover of each species per plot (e.g. Cover) and two columns with information about the hierarchy (i.e. Site and PlotID).\n\n#> # A tibble: 110 × 4\n#>    Taxon             Cover Site  PlotID\n#>    <chr>             <dbl> <chr> <chr> \n#>  1 alopecurus ovatus   0.5 1     B     \n#>  2 alopecurus ovatus   1   1     C     \n#>  3 alopecurus ovatus   1   1     D     \n#>  4 alopecurus ovatus   2   1     F     \n#>  5 alopecurus ovatus   0.1 1     G     \n#>  6 bistorta vigdis    10   1     A     \n#>  7 bistorta vigdis    25   1     B     \n#>  8 bistorta vigdis    10   1     C     \n#>  9 bistorta vigdis     2   1     D     \n#> 10 bistorta vigdis     1   1     F     \n#> # … with 100 more rows\n\nThe trait data should contain information about traits and trait values for as many species and individuals in the community data as possible. The data should be organized in the same way as the community data and should have corresponding columns. In this example the trait data contains Taxon, Site and PlotID as well as Trait and Value.\n\n#> # A tibble: 749 × 5\n#>    Taxon                   Site  PlotID Trait         Value\n#>    <chr>                   <chr> <chr>  <chr>         <dbl>\n#>  1 saxifraga oppositifolia 1     A      Wet_Mass_g 0.000695\n#>  2 bistorta vigdis         1     C      Wet_Mass_g 0.0105  \n#>  3 festuca rubra           2     C      Wet_Mass_g 0.00724 \n#>  4 bistorta vigdis         1     C      Wet_Mass_g 0.0189  \n#>  5 equisetum arvense       1     E      Wet_Mass_g 0.270   \n#>  6 bistorta vigdis         1     B      Wet_Mass_g 0.0231  \n#>  7 luzula confusa          1     F      Wet_Mass_g 0.0135  \n#>  8 alopecurus ovatus       1     G      Wet_Mass_g 0.0234  \n#>  9 alopecurus ovatus       1     G      Wet_Mass_g 0.0300  \n#> 10 alopecurus ovatus       1     C      Wet_Mass_g 0.0267  \n#> # … with 739 more rows"
  },
  {
    "objectID": "11_bootstrap.html#trait-imputation",
    "href": "11_bootstrap.html#trait-imputation",
    "title": "\n11  Bootstrapping trait data\n",
    "section": "\n11.4 Trait imputation",
    "text": "11.4 Trait imputation\nThe trait_impute function uses a hierarchical sampling design, which allows to account for incomplete trait collections, traits from different spatial or temporal levels (i.e. local traits vs. databases), different taxonomic resolution and/or experimental design.\nThe first two mandatory arguments in the function are the two datasets: comm and traits\nThe next four arguments are also mandatory and refer to specific columns in the trait or community dataset:\n\n\nabundance which is the abundance of your species in your community dataset. This can be abundance, cover, biomass, or size, etc.\n\ntaxon_col is the column in your community and trait data that define the species.\n\ntrait_col is the column in your trait data that defines the traits.\n\nvalue_col is the column in your trait data that defines the trait values.\n\nAll the other arguments are not mandatory.\nWith scale_hierarchy you can define the levels at which the traits have been collected and the order of trait imputation starting with the highest level (e.g. global database, regional, site, plot). In the example below we have the levels Site and PlotID, starting with the highest level.\nThe trait_impute function will choose if available a trait value from the lowest level, i.e. species X from plot A in site 1. If no trait value is available from that level (plot A, site 1), it will other groups in the same level and choose a trait value from species X from plot B or C at site 1. If there is no trait available, it will move up the hierarchy to the next level and choose trait values from species X from other sites (site 2, 3, etc.).\nThe argument min_n_in_samples allows users to define the minimum number in samples that are chosen at each level. If the minimum number is not reached (i.e. there are only 3 trait values at a specific level), trait values from the next higher level will be imputed, to avoid sampling the same individual several times, which could result in unrealistic variances. The default minimum number of samples is 5.\nIn the other_col argument other grouping variables in the community dataset can be defined and will be kept after the trait imputation.\n\ntrait_imputation <- trait_impute(\n    # input data (mandatory)\n    comm = community,\n    traits = trait,\n    \n    # specifies columns in your data (mandatory)\n    abundance_col = \"Cover\",\n    taxon_col = \"Taxon\",\n    trait_col = \"Trait\",\n    value_col = \"Value\",\n    \n    # specifies sampling hierarchy\n    scale_hierarchy = c(\"Site\", \"PlotID\"),\n    \n    # min number of samples\n    min_n_in_sample = 9\n  )\ntrait_imputation\n#> # A tibble: 4,007 × 12\n#> # Groups:   global, Site, PlotID, Trait [28]\n#>    Taxon  Cover Site  PlotID global sum_abun Trait   Value n_sample weight level\n#>    <chr>  <dbl> <chr> <chr>  <chr>     <dbl> <chr>   <dbl>    <int>  <dbl> <ord>\n#>  1 festu…     1 1     A      global     38.2 Wet_… 0.00724       11 0.0909 glob…\n#>  2 festu…     1 1     A      global     38.2 Wet_… 0.0160        11 0.0909 glob…\n#>  3 festu…     1 1     A      global     38.2 Wet_… 0.00529       11 0.0909 glob…\n#>  4 festu…     1 1     A      global     38.2 Wet_… 0.0154        11 0.0909 glob…\n#>  5 festu…     1 1     A      global     38.2 Wet_… 0.0066        11 0.0909 glob…\n#>  6 festu…     1 1     A      global     38.2 Wet_… 0.00637       11 0.0909 glob…\n#>  7 festu…     1 1     A      global     38.2 Wet_… 0.00491       11 0.0909 glob…\n#>  8 festu…     1 1     A      global     38.2 Wet_… 0.0132        11 0.0909 glob…\n#>  9 festu…     1 1     A      global     38.2 Wet_… 0.0233        11 0.0909 glob…\n#> 10 festu…     1 1     A      global     38.2 Wet_… 0.0115        11 0.0909 glob…\n#> # … with 3,997 more rows, and 1 more variable: max_n_in_sample <int>\n\nTraitstrap also allows to include taxonomy and experimental design in the trait imputation step.\nWith the argument taxon_col the taxonomic hierarchy for sampling can be defined. This means if traits for a specific species are not available, trait values from the same genus will be imputed. For this a list of the taxonomic hierarchy has to be defined (e.g. “Taxon”, “Genus”). Note that traits from species of the same genus can have very different traits and it might not be meaningful to impute these traits. Therefore, you should always check the trait distributions for the same genus before using taxonomic trait imputation.\nThe argument treatment_col allows to incorporate an experimental design where traits are imputed from the same experimental treatment or the first factor level, which is assumed to be the control. Therefore, it is important to order the levels of a treatment in the right order, i.e. the first level has to be the control. The imputation step can be defined at certain level using the treatment_level argument. Depending on the experimental design it might make sense to impute traits at a certain level, e.g. block or site.\nHere is an example how to include taxonomy and experimental design in the trait imputation function (code not run).\n\ntrait_imputation2 <- trait_impute(\n    comm = community,\n    traits = trait,\n    \n    abundance_col = \"Cover\",\n    \n    # defining taxonomic hierarchy\n    taxon_col = c(\"Taxon\", \"Genus\"),\n    \n    trait_col = \"Trait\",\n    value_col = \"Value\",\n    \n    scale_hierarchy = c(\"Site\", \"PlotID\"),\n    min_n_in_sample = 3\n    \n    # specifying experimental design\n    treatment_col = \"Treatment\",\n    treatment_level = \"Site\",\n  )"
  },
  {
    "objectID": "11_bootstrap.html#nonparametric-bootstrapping",
    "href": "11_bootstrap.html#nonparametric-bootstrapping",
    "title": "\n11  Bootstrapping trait data\n",
    "section": "\n11.5 Nonparametric bootstrapping",
    "text": "11.5 Nonparametric bootstrapping\nThe output of the trait imputation function is then used to do a nonparametric bootstrapping using the trait_np_bootstrap function.\nNonparametric bootstrapping is a resampling method to estimate the trait moments. The traits are re-sampled in proportion to their weight in the community (e.g. by the abundance of the species).\nThe trait values across all individuals in a community are resampled n times (sample_size; the default is 200) to incorporate the full spectrum of trait variation, generating n number (nrep; the default is 100) of trait distributions.\nFrom these trait distributions the happy moments are estimated: mean, variance, skewness and kurtosis.\nThis function also allows to extract raw distributions by setting the argument raw = TRUE. The raw data can be useful for visualizing the trait distributions. If the raw data is extracted, nrep is forced to 1 to avoid memory issues.\n\n# run nonparametric bootstrapping\nnp_bootstrapped_moments <- trait_np_bootstrap(\n  trait_imputation, \n  nrep = 200\n  )\nnp_bootstrapped_moments\n#> # A tibble: 5,600 × 9\n#> # Groups:   global, Site, PlotID [14]\n#>    n     global Site  PlotID Trait             mean   variance skewness kurtosis\n#>    <chr> <chr>  <chr> <chr>  <chr>            <dbl>      <dbl>    <dbl>    <dbl>\n#>  1 1     global 1     A      Plant_Height_cm 1.61    2.15         3.14    12.1  \n#>  2 1     global 1     A      Wet_Mass_g      0.0121  0.0000655    1.06     1.58 \n#>  3 1     global 1     B      Plant_Height_cm 1.80    2.72         2.64     8.37 \n#>  4 1     global 1     B      Wet_Mass_g      0.0141  0.0000616    0.771    0.953\n#>  5 1     global 1     C      Plant_Height_cm 1.84    2.27         2.29     5.29 \n#>  6 1     global 1     C      Wet_Mass_g      0.0139  0.0000742    1.66     4.48 \n#>  7 1     global 1     D      Plant_Height_cm 1.82    4.33         2.66     6.76 \n#>  8 1     global 1     D      Wet_Mass_g      0.0123  0.0000692    4.18    27.8  \n#>  9 1     global 1     E      Plant_Height_cm 7.22   20.8          0.272   -0.723\n#> 10 1     global 1     E      Wet_Mass_g      0.0150  0.000178     3.37    12.4  \n#> # … with 5,590 more rows\n\nOne advantage of using a bootstrapping approach, is that we get much more than a mean trait value. We can also estimate the variance and other moments of these trait distributions. In traitstrap happy moments can be summarized and the confidence intervals calculated using the trait_summarise_boot_moments function. The input variable for this function is the output from the nonparametric bootstrapping function (or the parametric bootstrapping function, see below).\nThe confidence interval can be calculated parametrically, using the mean and standard deviation, or nonparametrically using quantiles. The default is using the mean and standard deviation (parametric = TRUE) with one standard deviation around each trait moment (sd_mult = 1). For the nonparametric approach the default is a 0.95 confidence level.\n\n# summarizes bootstrapping output\nsum_boot_moment <- trait_summarise_boot_moments(\n  np_bootstrapped_moments\n  )\nsum_boot_moment\n#> # A tibble: 28 × 17\n#> # Groups:   global, Site, PlotID [14]\n#>    global Site  PlotID Trait           n   mean ci_low_mean ci_high_mean     var\n#>    <chr>  <chr> <chr>  <chr>       <int>  <dbl>       <dbl>        <dbl>   <dbl>\n#>  1 global 1     A      Plant_Heig…   200 1.71        1.60         1.82   2.61e+0\n#>  2 global 1     A      Wet_Mass_g    200 0.0121      0.0115       0.0127 6.65e-5\n#>  3 global 1     B      Plant_Heig…   200 1.76        1.65         1.86   2.66e+0\n#>  4 global 1     B      Wet_Mass_g    200 0.0136      0.0130       0.0142 6.97e-5\n#>  5 global 1     C      Plant_Heig…   200 1.73        1.63         1.83   1.86e+0\n#>  6 global 1     C      Wet_Mass_g    200 0.0149      0.0141       0.0158 1.27e-4\n#>  7 global 1     D      Plant_Heig…   200 1.88        1.74         2.01   4.72e+0\n#>  8 global 1     D      Wet_Mass_g    200 0.0118      0.0112       0.0123 4.90e-5\n#>  9 global 1     E      Plant_Heig…   200 7.04        6.74         7.34   1.72e+1\n#> 10 global 1     E      Wet_Mass_g    200 0.0159      0.0146       0.0172 2.62e-4\n#> # … with 18 more rows, and 8 more variables: ci_low_var <dbl>,\n#> #   ci_high_var <dbl>, skew <dbl>, ci_low_skew <dbl>, ci_high_skew <dbl>,\n#> #   kurt <dbl>, ci_low_kurt <dbl>, ci_high_kurt <dbl>"
  },
  {
    "objectID": "11_bootstrap.html#parametric-bootstrapping",
    "href": "11_bootstrap.html#parametric-bootstrapping",
    "title": "\n11  Bootstrapping trait data\n",
    "section": "\n11.6 Parametric bootstrapping",
    "text": "11.6 Parametric bootstrapping\nTraitstrap also offers the option to run a parametric bootstrapping.\nThe trait_fit_distributions function fits parametric distributions for each species-by-trait combination at the finest scale of the user-supplied hierarchy. This function takes as input:\n\nan object of class imputed traits (as produced by the function trait_impute), and\nthe type of distribution to be fitted.\n\nEither a single distribution type can be used for all traits, or traits can be assigned specific distributions types by supplying the function with a named list of traits (e.g. list(height = \"normal\", mass = \"lognormal\")).\nCurrently the function supports normal, log-normal, and beta (values between 0 and 1) distributions.\nThe function returns a dataframe containing fitted distribution parameters.\n\n# fit distributions\nfitted_distributions <- trait_fit_distributions(\n  imputed_traits = trait_imputation,\n  distribution_type = \"lognormal\"\n  )\n#> Warning in .data[[\"Trait\"]] == names(distribution_type)[distribution_type == :\n#> longer object length is not a multiple of shorter object length\nfitted_distributions\n#> # A tibble: 202 × 15\n#> # Groups:   global, Site, PlotID, Trait, Taxon, Cover, n_sample [202]\n#>    global Site  PlotID Trait  Taxon Cover n_sample distribution_ty…  parm1 parm2\n#>    <chr>  <chr> <chr>  <chr>  <chr> <dbl>    <int> <chr>             <dbl> <dbl>\n#>  1 global 1     A      Plant… bist…  10         28 lognormal         0.328 0.517\n#>  2 global 1     A      Plant… drya…   0.1        9 lognormal         1.04  0.704\n#>  3 global 1     A      Plant… fest…   1         11 lognormal         1.61  0.377\n#>  4 global 1     A      Plant… luzu…   0.5       15 lognormal         1.50  0.325\n#>  5 global 1     A      Plant… luzu…   1         20 lognormal         0.588 0.459\n#>  6 global 1     A      Plant… sali…  20         44 lognormal        -0.138 0.931\n#>  7 global 1     A      Plant… saxi…   2          6 lognormal         1.13  0.469\n#>  8 global 1     A      Plant… saxi…   2          2 lognormal         0.693 0    \n#>  9 global 1     A      Plant… sile…   1          3 lognormal        -0.221 0.277\n#> 10 global 1     A      Wet_M… bist…  10         29 lognormal        -4.06  0.454\n#> # … with 192 more rows, and 5 more variables: sd1 <lgl>, sd2 <lgl>, ks <dbl>,\n#> #   cvm <dbl>, ad <dbl>\n\n\n# fit several types of distributions\nfitted_distributions <- trait_fit_distributions(\n  imputed_traits = trait_imputation,\n  distribution_type = list(Plant_Height_cm = \"normal\", Wet_Mass_g = \"lognormal\")\n  )\nfitted_distributions\n\nThe trait_parametric_bootstrap function is a parametric analogue of the trait_np_bootstrap function. It takes in fitted trait distributions produced by trait_fit_distributions and randomly samples from among the fitted distributions proportionally to species abundances in the community.\nAs with trait_np_bootstrap, the number of samples per replicated draw are specified with the parameter sample_size, and the number of replicated draws is specified by the parameter nrep. The argument raw allows to extract raw distributions (see above).\n\n# run parametric bootstrapping\np_bootstrapped_moments <- trait_parametric_bootstrap(\n    fitted_distributions = fitted_distributions, \n    nrep = 200\n    )\np_bootstrapped_moments\n#> # A tibble: 5,600 × 9\n#> # Groups:   global, Site, PlotID, Trait [28]\n#>    n     global Site  PlotID Trait             mean   variance skewness kurtosis\n#>    <chr> <chr>  <chr> <chr>  <chr>            <dbl>      <dbl>    <dbl>    <dbl>\n#>  1 1     global 1     A      Plant_Height_cm 1.61    1.39         1.59     3.36 \n#>  2 1     global 1     A      Wet_Mass_g      0.0136  0.0000755    1.21     2.04 \n#>  3 1     global 1     B      Plant_Height_cm 1.64    2.03         2.00     4.99 \n#>  4 1     global 1     B      Wet_Mass_g      0.0142  0.0000693    1.12     1.25 \n#>  5 1     global 1     C      Plant_Height_cm 1.73    1.54         2.58     7.92 \n#>  6 1     global 1     C      Wet_Mass_g      0.0150  0.000190     5.89    45.2  \n#>  7 1     global 1     D      Plant_Height_cm 1.92    5.40         2.64     7.87 \n#>  8 1     global 1     D      Wet_Mass_g      0.0122  0.0000475    2.02     7.49 \n#>  9 1     global 1     E      Plant_Height_cm 7.09   15.4          0.330    0.291\n#> 10 1     global 1     E      Wet_Mass_g      0.0150  0.000206     2.49     7.44 \n#> # … with 5,590 more rows\n\nThe output of trait_parametric_bootstrap can be summarized using trait_summarize_boot_moments (see above)."
  },
  {
    "objectID": "11_bootstrap.html#extracting-raw-distributions",
    "href": "11_bootstrap.html#extracting-raw-distributions",
    "title": "\n11  Bootstrapping trait data\n",
    "section": "\n11.7 Extracting raw distributions",
    "text": "11.7 Extracting raw distributions\nIn traitstrap both the parametric and nonparametric bootstrapping functions allow returning raw trait distributions.\n\n# run nonparametric bootstrapping\nraw_dist_np <- trait_np_bootstrap(\n  trait_imputation,\n  raw = TRUE\n  )\nraw_dist_np\n#> # A tibble: 5,600 × 13\n#> # Groups:   global, Site, PlotID, Trait [28]\n#>    n     Taxon    Cover Site  PlotID global sum_abun Trait Value n_sample weight\n#>    <chr> <chr>    <dbl> <chr> <chr>  <chr>     <dbl> <chr> <dbl>    <int>  <dbl>\n#>  1 1     salix p…    20 1     A      global     38.2 Plan…   0.5       44  0.455\n#>  2 1     silene …     1 1     A      global     38.2 Plan…   1          3  0.333\n#>  3 1     salix p…    20 1     A      global     38.2 Plan…   0.3       44  0.455\n#>  4 1     luzula …     1 1     A      global     38.2 Plan…   2.4       20  0.05 \n#>  5 1     bistort…    10 1     A      global     38.2 Plan…   0.8       28  0.357\n#>  6 1     salix p…    20 1     A      global     38.2 Plan…   0.7       44  0.455\n#>  7 1     salix p…    20 1     A      global     38.2 Plan…   1.1       44  0.455\n#>  8 1     salix p…    20 1     A      global     38.2 Plan…   1.6       44  0.455\n#>  9 1     salix p…    20 1     A      global     38.2 Plan…   1         44  0.455\n#> 10 1     saxifra…     2 1     A      global     38.2 Plan…   2          2  1    \n#> # … with 5,590 more rows, and 2 more variables: level <ord>,\n#> #   max_n_in_sample <int>\n\nThe raw data can be useful for visualizing the trait distributions.\nUse colour and facets to separate between the different traits, hierarchies and treatments.\n\nggplot(raw_dist_np, aes(x = log(Value), fill = Site)) +\n  geom_density(alpha = 0.4) +\n  scale_fill_viridis_d(end = 0.9, option = \"plasma\") +\n  labs(x = \"log(trait value)\") +\n  facet_wrap( ~ Trait, scales = \"free\")"
  },
  {
    "objectID": "11_bootstrap.html#check-your-data",
    "href": "11_bootstrap.html#check-your-data",
    "title": "\n11  Bootstrapping trait data\n",
    "section": "\n11.8 Check your data",
    "text": "11.8 Check your data\nTraitstrap has a couple of functions to check the data.\nThe coverage_plot function shows the trait coverage of the community for each level. Basically, this function summarizes from which level the traits are imputed, and how much coverage of the community is reached.\nBased on simulations, we recommend to collect traits for at least 80% of the community cover (Maitner et al. in prep).\n\n# show coverage plot\nautoplot(trait_imputation) + \n  theme(axis.text.x = element_text(size = 8, angle = 90, vjust = 0.5))\n\n\n\n\nAnother important information is to know of which taxa traits are missing. This can be useful if the data sampling is not finished and you want to know which species should be sampled. The function also tells you the maximal abundance of each missing species, and gives you useful information if the missing species are abundant or rare.\nTraitstrap has a function trait_missing which gives you a table with all missing values.\n\n# list missing traits\ntrait_missing(imputed_trait = trait_imputation,\n              comm = community)\n#> # A tibble: 20 × 3\n#> # Groups:   Taxon [20]\n#>    Taxon                      max_abun n_traits\n#>    <chr>                         <dbl>    <int>\n#>  1 alopecurus ovatus               2          2\n#>  2 bistorta vigdis                25          2\n#>  3 calamagrostis neglecta         60          2\n#>  4 cassiope tetragona              5          2\n#>  5 dryas octopetala               20          2\n#>  6 enquistetum scirpoides          2          2\n#>  7 festuca rubra                   1          2\n#>  8 juncus biglumis                 0.5        1\n#>  9 luzula confusa                  5          2\n#> 10 luzula nivalis                  5          2\n#> 11 maitneranthes hieracifolia      0.5        1\n#> 12 oxyria tanyna                   2          2\n#> 13 poa pratensis                   1          2\n#> 14 salix polaris                  43          2\n#> 15 saxifraga hirculus              2          2\n#> 16 saxifraga oppositifolia         2          2\n#> 17 silene acaudis                  1          2\n#> 18 stelfordaria humifusa           0.5        1\n#> 19 stellaria longipes              0.1        1\n#> 20 trisetum spicatum               3          2"
  }
]