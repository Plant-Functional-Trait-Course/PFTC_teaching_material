```{r, setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

library(tidyverse)
library(lubridate)

```


## Data curation

Data curation, transformation or cleaning is the first step after digitizing the data.
Each dataset has to be checked for errors and corrected as best as possible.
This tutorial shows how to check your dataset for errors and how to correct them.

For this tutorial we will be working with the trait dataset from Svalbard.
See section \@ref(working-with-pftc-data) for how to access the data and information about the study, experiment and datasets.


### Useful packages

There are a couple of R packages that are useful for this work. 
First, **tidyverse** is a collection of R packages used for basic data manipulation and analysis.

If you have never used the packages you need to install it first using the function  `install.packages("tidyverse")`.
Otherwise, you can just load the packages.

```{r, load-packages, eval=FALSE}

library(tidyverse)

```

Second, another useful package for data curation is **tidylog**, which is built on the **dplyr** and **tidyr** packages and provides useful information about the functions used.

Tidylog will for example tell you how many rows have been removed and are remaining when using the `filter()` function or how many rows match when using a join function.
The information is always indicated in absolute numbers and percentage.
The additional information is very useful to check if the right observations have been removed or manipulated, because mistakes are easily done.

Let's install and/or load **tidylog**.

```{r, load-tidylog, message=FALSE}

library(tidylog)

```


Note, that once **tidylog** is loaded it will automatically prioritize the tidylog function before the dplyr and tidyr functions.
You actively have to choose if you do not want to use the tidylog version by using this notation: `dplyr::filter()`.


### Import data

The first step is to import the data to R.
The data is stored as a _csv file_ and we can use the function `read_csv()` to import that data.
If your data has another format or importing data is new to, you have a look at this [page](https://biostats-r.github.io/biostats/workingInR/importing-data-in-r.html).

```{r, trait-import}

raw_traits <- read_csv("data/PFTC4_Svalbard_2018_Gradient_Traits.csv")

```

Give the dataset a name that indicates that this is the raw data.

The dataset contains measurements of 14 traits from two elevational gradients on Svalbard.
The traits were measured on individual plants from 21 different graminoid and forb species.
For more information about the sites, traits and measurements see [here](https://github.com/Plant-Functional-Trait-Course/PFTC_4_Svalbard).


**Some manipulation**

Let us introduce some errors to the dataset.

The code to do this is hidden.
But if you want to replicate the code to introduce errors you can find the code here (add link!!!).


```{r, introduce-errors, echo=FALSE, message=FALSE}

raw_traits <- raw_traits |> 
  # simplify
  select(-Project, -Functional_group, -Year) |> 
  #introduce errors
  # wrong date
  mutate(Date = as.character(Date),
         Date = if_else(ID == "AMO3822", "18", Date),
         # giant leaf
         Value = if_else(ID == "ANH3472" & Trait == "Leaf_Area_cm2", 17965, Value),
         # diverse species names
         Taxon = case_when(ID == "BON2388" ~ "oxiria digyna",
                           ID == "ATX7216" ~ "oxyria digina",
                           ID == "BSV7581" ~ "oxyra digyna",
                           TRUE ~ Taxon),
         # introduce NAs
         Value = if_else(ID %in% c("BLQ1061", "AZO1656", "AMV4451") & Trait == "LDMC", NA_real_, Value))
  
# add duplicate row
duplicate <- raw_traits |> 
  slice(378)

# introduce dry weight measurements with wrong unit (mg instead of g)
wrong_decimal <- raw_traits |> 
  filter(Trait == "Dry_Mass_g") |> 
  sample_n(size = 50, replace = FALSE) |> 
  mutate(Value2 = Value / 1000)

raw_traits <- raw_traits |> 
  left_join(wrong_decimal, by = c("Date", "Gradient", "Site", "PlotID", "Individual_nr", "ID", "Taxon", "Trait", "Value", "Elevation_m", "Latitude_N", "Longitude_E")) |> 
  mutate(Value = if_else(!is.na(Value2), Value2, Value)) |> 
  select(-Value2) |> 
  bind_rows(duplicate)

```


### First check of the dataset

By typing `raw_traits` in the console it will display the first rows and columns of the dataset.

```{r, display-table}

raw_traits

```

At the top you can see that the dataset has `r dim(raw_traits)[1]` observations and `r length(raw_traits)` columns.
These numbers give you a first impression if you have imported the right file, and if all your observations and columns are there.


### Check the data type

The next thing to check is if the variables have the right data type (or class in R terminology).
For each variable the output indicates the data type. The most common types are _dbl_ (numeric or integer), _chr_ (character), or _date_ (date).

If you want to know more about data types see [here](https://biostats-r.github.io/biostats/workingInR/first-steps-in-r.html#data-types-and-objects).

Now we want to check if the variables have the right data type.
The first variable *Date* is a character, which is not correct.
This probably means that one or several observations have a wrong date.
Let us check all different values for the variable *Date*.
For this we can use the function `distinct()` on the variable *Date*.

```{r, distinct}

raw_traits |> 
  distinct(Date)

```

We can see that there are 6 distinct dates in this variable.
One of the dates is "18", which is not a correct date format and turned the variable into a character.
Note that the additional information from the tidylog package about the `distinct()` function.
It shows the number of rows removed and remaining.

The next step is to check where the problem occurred.
For this we can use the function `filter()` to extract all observations with the date 18.
We can use `as.data.frame()` to display the whole table.

```{r, filter, eval=FALSE}

raw_traits |> 
  filter(Date == "18") |> 
  View()

```


```{r, filter-print, echo=FALSE}

raw_traits |> 
  filter(Date == "18") |> 
  print()


```


We see that it is a single observation (with multiple traits) that has the wrong date.
The next step is to check the raw data, notes, photos, etc. to find the correct date for this observations.
It is important to keep the data entry sheets, take a photo of them and keep the field notes to be able to fix such problems.

<!-- Add picture of the envelope!!! -->

Since the value for date is 18, we will assume now that this was a typo and that the correct date is 2018-07-18.
Let's replace this value and give the variable the right class.

For this we will use the function `mutate()` which adds or manipulates a column.
Inside the mutate we will use the `case_when()` function to replace the date for a specific ID.
We are using the function with a single statement, however this is a powerful function that allow for multiple statements (many if else conditions).
To give the variable the correct class, we use the `ymd()` function from the lubridate package.
Note that we now have to assign the table to a new or the same name to make the change permanent.

```{r, replace-date}

raw_traits <- raw_traits |> 
  mutate(Date = case_when(ID == "AMO3822" ~ "2018-07-18",
                          TRUE ~ Date)) |> 
  mutate( Date = ymd(Date))
        

```


An important step when cleaning data is to check that you have done the right thing.
The **tidylog** functions show that for 7 observation Date has been changed.
This matches with the number of observations that had a wrong date.

To be sure we can look at the specific leaf (ID == "AMO3822") and see if the date is now corrected.
Another way would be to run the `distinct(Date)` function again. 

```{r, chacke-date}

raw_traits |> 
  filter(ID == "AMO3822") |> 
  select(Date)

```

The date has been fixed.

### Exercise {- .exercise .toc-ignore}

Now it is your turn.
Check if the data type for the variable *Date* is now correct.

<details>
  <summary>Hint</summary>
- type `raw_traits` to look at the whole dataset where the datatype of each variable is indicated
- use `class(raw_traits$Date)` which will tell you directly what type of class the variable has
- use `map(raw_traits, class)` to get the class of all variable in the dataframe
</details>


### Check for duplicates

Another common problem is duplicate observations.
This can happen when data is entered twice.
The why to find duplicates is to check that the combination of variables are unique.
In our dataset, we expect that *Date*, *Gradient*, *Site*, *PlotID*, *Individual_nr*, *ID*, *Taxon* and *Trait* should be unique, and only occurring once.

To check this, we can `group_by()` these variables and `filter()` for observations that occur more than once.

```{r, check-duplicates}

raw_traits |> 
  group_by(Date, Gradient, Site, PlotID, Individual_nr, ID, Taxon, Trait) |> 
  filter(n() > 1)

```

There is one duplicate entry.

Note that *Value* was not included in the `group_by()`.
This was done intentionally, because a common mistake is to have a duplicate, but with a different value.
This is either because one of the variables is wrong, e.g. it has the wrong Site and therefore appears to be a duplicate.
Alternatively, the leaf could have been measured twice by accident, which would likely give two slightly different values.
When getting a duplicate, these different options for why there is a duplicate have to be considered and carefully checked in the raw data.

In this case, we will assume that the leaf has only been measured once, but the data has been entered twice.
Thus, the two entries are exact duplicates.

To fix the duplicate problem, we group by the variables we expect to be unique.
Then we use `distinct()` with the argument *.keep_all = TRUE* to remove the duplicates.


```{r, remove-duplicates}

raw_traits2 <- raw_traits |> 
  group_by(Date, Gradient, Site, PlotID, Individual_nr, ID, Taxon, Trait) |> 
  distinct(.keep_all = TRUE)

```

Tidylog allows us to see what happens and how many rows have been removed.
There are 8 grouping variables and as expected, one row is filtered away, which is the duplicated row.

We can also run the code from above again to check if the duplicate is gone.


### Check for missing data

A common problem in a dataset are missing values.

How to detect missing values...

Once the missing values are detected one has to decide if the missing data can be recovered, or if the missing values should be removed from the dataset.


```{r, remove-na}

raw_traits <- raw_traits |> 
  drop_na(Value)

```

This operation has removed 3 rows, which is the number of NA's in the dataset.


### Check values within variables

```{r, unique-names}

raw_traits |> 
  distinct(Taxon) |> 
  arrange(Taxon) |> 
  print(n = Inf)

```

4 different versions of *oxyra digyna*!
rename
alternative would be to use a dictionary.

```{r, fix-oxyra}

raw_traits <- raw_traits |> 
  mutate(Taxon = if_else(Taxon %in% c("oxiria digyna", "oxyria digina", "oxyra digyna"), "oxyria digyna", Taxon))

```


### Visualise data

Some errors and problems in the data are difficult to detect.
Checking if the measurements are realistic is nearly impossible by looking at a dataframe.
For this, visualising the data is much more effective.

Make histograms, this shows the range of the values

```{r, hist}
raw_traits |> 
  ggplot(aes(x = Value, fill = Gradient)) +
  geom_density(alpha = 0.7) +
  scale_fill_manual(values = c("green4", "grey")) +
  facet_wrap(~ Trait, scales = "free")

```


Note that the size traits (height, mass, area and thickness) have very long tails.
It is common to log transform such variables to better see the full range of the variables.

Leaf area has a huge tail and goes up to almost 20'000 cm^2^.
This would be a leaf of almost 2 m^2^, which is nearly impossible.
This value needs to be checked, it could be a typo.

Let's log transform the size traits.

```{r, log-transform}

raw_traits <- raw_traits |> 
  mutate(Value = if_else(Trait %in% c(
    "Plant_Height_cm",
    "Wet_Mass_g",
    "Dry_Mass_g",
    "Leaf_Area_cm2",
    "Leaf_Thickness_mm"), log(Value), Value),
    Trait = recode(Trait,
                   "Plant_Height_cm" = "Plant_Height_cm_log",
                   "Wet_Mass_g" = "Wet_Mass_g_log",
                   "Dry_Mass_g" = "Dry_Mass_g_log",
                   "Leaf_Area_cm2" = "Leaf_Area_cm2_log",
                   "Leaf_Thickness_mm" = "Thickness_mm_log"))

```

And make plot again.

```{r, hist-log}
raw_traits |> 
  ggplot(aes(x = Value, fill = Gradient)) +
  geom_density(alpha = 0.7) +
  scale_fill_manual(values = c("green4", "grey")) +
  facet_wrap(~ Trait, scales = "free")

```

The size traits are now easier to read.

Another way to check the data is to plot correlated values against each other.
In this dataset, we can plot dry mass against leaf area. We would expect a positive correlation between the two variables, where large leaves have a higher dry mass.

```{r, plot, eval=FALSE}
raw_traits |> 
  pivot_wider(names_from = Trait, values_from = Value) |> 
  ggplot(aes(x = Dry_Mass_g_log, y = Leaf_Area_cm2_log)) +
  geom_point()

```

We can clearly see the outlier that has a 2 m^2^ leaf.
We can also see that there is a cloud of points that is separate from the rest of the data.
These data have a sensible leaf area but the dry weight is too small.
It is possible that they were measured in the wrong unit.

<!-- Add lines to the plot! -->

